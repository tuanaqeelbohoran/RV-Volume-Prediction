{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2 \n",
    "import glob\n",
    "from natsort import natsorted\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from natsort import natsorted\n",
    "import os, sys\n",
    "%matplotlib inline\n",
    "# demonstrate data standardization with sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# physical_devices = tf.config.list_physical_devices('GPU') \n",
    "# for device in physical_devices:\n",
    "#     tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "\n",
    "import torch\n",
    "# import torch.nn as nn\n",
    "from torch import nn\n",
    "# from tab_transformer_pytorch import TabTransformer  \n",
    "torch.backends.cudnn.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch is running on GPU\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"PyTorch is running on GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"PyTorch is running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_no =\"045\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Excel file\n",
    "df = pd.read_excel('Areas_of _echo.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    RV_Study_ID  FourC_ED  FourC_ES  PLAX_ED  PLAX_ES  PSAX_AV_ED  PSAX_AV_ES  \\\n",
      "0             1     13.54      5.70     6.94     3.13       23.95       14.35   \n",
      "1             2     16.05      5.45     8.46     4.45       16.33        8.68   \n",
      "2             3     10.76      4.52     6.67     3.54       13.87        6.07   \n",
      "3             4     18.39     12.33    12.32     5.77       26.27       15.35   \n",
      "4             5     20.36     11.83    11.87     7.16       32.87       18.25   \n",
      "5             6     10.68      5.97     8.64     4.90       17.08        7.19   \n",
      "6             7     22.62     14.50    11.77     8.03       21.46       13.70   \n",
      "7             8     13.08      8.76    17.18     9.57       16.61        8.59   \n",
      "8             9     20.81     11.80     8.71     4.64       17.35       10.12   \n",
      "9            10     13.57      4.47    14.00     6.20       15.98        2.72   \n",
      "10           11     16.80      7.59    14.01     6.10       26.62       14.48   \n",
      "11           12      7.75      3.48     7.88     4.22       16.76        5.72   \n",
      "12           13     20.45     11.36     8.00     2.88       26.84       12.34   \n",
      "13           14     17.81      8.30     9.93     3.99       25.31        7.22   \n",
      "14           15     14.21      6.93     9.25     5.58       17.97        9.07   \n",
      "15           16     25.78     15.49    13.66     7.12       27.55       14.88   \n",
      "16           17     23.97     19.17    13.33     9.18       16.70        8.51   \n",
      "17           18     37.86     20.43    24.52    13.11       41.64       20.52   \n",
      "18           19     11.86      6.29     9.80     4.67       16.75        6.64   \n",
      "19           20     20.41      8.50    11.41     3.89       14.30        6.34   \n",
      "20           21     19.91      9.78    13.14     6.23       35.06       21.12   \n",
      "21           22     14.66      9.80     6.19     3.44       10.52        6.93   \n",
      "22           23     16.85      6.59    12.88     7.24       14.13        8.58   \n",
      "23           24     14.34      6.48    10.16     3.98       12.05        1.36   \n",
      "24           25     18.10      8.61    12.23     3.86       21.51       10.09   \n",
      "25           26     14.53      8.73    17.77     8.40       28.13       15.16   \n",
      "26           27     21.10     11.05    13.34     7.46       24.35       15.21   \n",
      "27           28     11.20      6.78     7.83     3.73       19.24       10.87   \n",
      "28           29     17.54     11.80    12.68     6.02       28.69       17.36   \n",
      "29           30     12.45      5.09     7.27     3.93       12.85        7.66   \n",
      "30           31     22.08     12.86     7.54     6.57       24.26       14.21   \n",
      "31           32     33.12     20.39    17.53    10.06       46.66       29.80   \n",
      "32           33     31.55     18.12    10.01     8.39       12.98        4.02   \n",
      "33           34     13.78      6.17    13.09     4.26       27.09       11.44   \n",
      "34           35     16.38     10.51    16.03     7.60       27.25       12.18   \n",
      "35           36     29.08     18.68    14.53     9.26       16.74        6.97   \n",
      "36           37     25.99     15.78    18.02    15.23       29.69       21.23   \n",
      "37           38     31.77     18.06    17.52     9.18       22.53       16.83   \n",
      "38           39     19.00     11.43    11.88     5.04       27.52       14.12   \n",
      "39           40     16.61      9.26     7.37     4.47       26.96       15.88   \n",
      "40           41     18.14      7.72    10.91     4.32       21.28       12.81   \n",
      "41           42     17.62      9.23    10.33     5.77       17.70       12.40   \n",
      "42           43     21.38     13.32    11.79     6.82       34.65       16.27   \n",
      "43           44     10.01      5.43     6.89     3.91       24.40       16.63   \n",
      "44           45     35.96     22.18    10.83     6.82       37.25       28.08   \n",
      "45           46     16.65     12.51     9.94     6.44       26.36       15.73   \n",
      "46           47     13.23      7.53    11.70     3.69       15.78        5.18   \n",
      "47           48     10.07      5.03     9.60     5.10       20.28        3.74   \n",
      "48           49     11.41      5.97     9.19     3.61       23.86       14.84   \n",
      "49           50     11.44      5.62     7.21     2.48       20.27       10.10   \n",
      "\n",
      "    PSAXbase_ED  PSAXbase_ES  PSAXdistal_ED  PSAXdistal_ES  PSAXmid_ED  \\\n",
      "0          9.33         7.03           2.03           0.78      11.140   \n",
      "1         19.34        16.03          12.04           8.85       9.520   \n",
      "2         11.59         8.65           1.79           0.53       6.030   \n",
      "3         11.43         7.84           4.70           3.05       6.770   \n",
      "4         20.40        15.34           4.04           1.54       9.170   \n",
      "5          5.98         3.96           0.62           0.21       2.890   \n",
      "6         14.98         8.99           8.18           4.28      11.030   \n",
      "7         15.45        10.61           3.25           1.73       6.710   \n",
      "8         12.31         9.84           1.94           1.15       4.750   \n",
      "9         18.27        11.35           6.65           5.17       9.050   \n",
      "10         9.44         4.40           1.80           0.31       5.770   \n",
      "11        11.48         9.21           4.15           2.82       6.680   \n",
      "12        16.64        11.23           6.10           3.45      19.860   \n",
      "13        11.51         6.55           3.91           1.65       8.700   \n",
      "14        13.38        10.39           0.90           0.60       3.790   \n",
      "15        28.08        21.24          13.52          10.01      18.650   \n",
      "16        13.23        10.78           8.14           5.87      10.970   \n",
      "17        41.36        31.58          21.18          13.53      40.090   \n",
      "18        15.39        11.57           1.78           0.87       6.720   \n",
      "19        19.93        12.65           4.16           1.95      12.390   \n",
      "20         9.20         7.05           2.27           2.02       4.880   \n",
      "21         7.82         5.47           2.18           1.04       3.680   \n",
      "22        11.37         5.63           3.00           1.21       5.580   \n",
      "23        14.91        10.00           1.71           0.70       4.470   \n",
      "24        15.12         9.36           5.05           1.64       6.400   \n",
      "25        16.68        11.55           5.29           1.88       8.970   \n",
      "26        19.01        13.14           5.19           3.69      16.900   \n",
      "27        16.22        11.22           2.71           1.10       5.340   \n",
      "28        14.02        11.28           6.74           3.71      10.290   \n",
      "29        18.66        14.44           1.77           0.74       3.830   \n",
      "30        14.36        12.26           3.21           3.00       4.270   \n",
      "31        28.94        26.71          18.13          14.46      20.520   \n",
      "32        18.45        18.13           4.17           3.90      14.180   \n",
      "33         9.48         3.00           3.09           0.79       4.690   \n",
      "34        19.80        15.94           1.66           0.85       7.300   \n",
      "35        21.73        15.96          14.06           7.24      15.700   \n",
      "36        39.01        30.98           8.21           6.21      22.080   \n",
      "37        16.40        10.49           6.59           2.06      13.460   \n",
      "38        16.14        11.93           3.44           2.13       7.670   \n",
      "39         6.22         4.59           3.96           2.59       4.573   \n",
      "40        17.66        14.55           3.33           1.82       5.610   \n",
      "41        18.52        15.82           4.31           2.21      10.060   \n",
      "42        25.66        23.58          10.46           5.13      12.080   \n",
      "43         5.58         3.80           1.19           0.40       2.830   \n",
      "44        22.85        19.41          13.98          10.24      28.190   \n",
      "45         8.08         6.37           1.94           0.07       6.610   \n",
      "46        17.88        12.34           5.22           1.98      11.690   \n",
      "47        12.14         7.81           2.90           1.17       2.780   \n",
      "48        18.10        12.72           3.65           0.69       9.090   \n",
      "49        14.12         9.84           5.60           1.14      10.960   \n",
      "\n",
      "    PSAXmid_ES  Rinflow_ED  Rinflow_ES  SubC_ED  SubC_ES  \\\n",
      "0         1.37       26.35       12.17     5.32    1.950   \n",
      "1         6.52       24.15       10.96    10.89    4.150   \n",
      "2         3.64       26.31       13.12     9.92    3.370   \n",
      "3         3.64       29.88       19.72     8.95    2.580   \n",
      "4         6.99       20.99       12.27    10.00    4.800   \n",
      "5         1.23       15.13        8.36     7.01    1.890   \n",
      "6         5.68       25.82       16.14    25.82   16.140   \n",
      "7         3.07       31.41       14.75     9.66    5.150   \n",
      "8         2.90       31.05       19.85     7.81    2.190   \n",
      "9         5.16       21.32        8.73    13.30    3.710   \n",
      "10        3.34       25.06       15.08    10.21    4.650   \n",
      "11        4.48       27.35       12.51    13.07    6.290   \n",
      "12       13.03       19.86       13.03    30.47   14.450   \n",
      "13        4.18       19.41        6.68    11.84    5.020   \n",
      "14        1.83       22.74       14.34    12.82    9.590   \n",
      "15       14.96       38.89       20.43    30.05   18.410   \n",
      "16        8.41       28.94       16.00    22.92   15.250   \n",
      "17       30.83       30.97       18.30    29.22   13.380   \n",
      "18        4.81       15.39        4.18     7.70    1.740   \n",
      "19        8.64       31.15       14.17    12.18    1.800   \n",
      "20        4.03       26.31       14.56    11.04    5.070   \n",
      "21        2.14       10.81        7.41     8.17    3.470   \n",
      "22        2.61       46.15       33.05    17.12    7.270   \n",
      "23        2.12       27.09       11.91    17.08    8.010   \n",
      "24        3.00       32.20       12.97    18.84    5.110   \n",
      "25        4.74       33.79       16.88    17.53    6.313   \n",
      "26       13.21       20.92       11.90     8.33    4.930   \n",
      "27        2.98       36.70       21.49    13.30    4.930   \n",
      "28        7.61       21.75       16.30    10.51    2.680   \n",
      "29        1.91       18.28        9.79     5.05    1.630   \n",
      "30        4.04       35.66       23.92     9.33    3.370   \n",
      "31       16.66       39.02       21.66    28.35   15.790   \n",
      "32       13.04       16.00       10.55    14.65    6.540   \n",
      "33        1.54       32.48       19.62    11.38    4.710   \n",
      "34        3.09       25.11       17.57    15.11    4.840   \n",
      "35       10.91       38.79       22.82    15.10    7.860   \n",
      "36       18.58       49.41       33.49    22.32   12.580   \n",
      "37        5.37       24.19       15.72    12.01    6.600   \n",
      "38        3.75       25.51       14.85    13.80   10.220   \n",
      "39        3.34       22.37       14.67     9.80    2.610   \n",
      "40        3.07       27.97       16.06     6.59    1.680   \n",
      "41        7.48       25.57       14.49    13.64    6.710   \n",
      "42       11.50       29.30       17.66    28.17   14.840   \n",
      "43        1.79       18.67       12.05     7.83    2.070   \n",
      "44       20.44       38.33       22.81    23.24   16.060   \n",
      "45        4.72       26.66       16.99     7.18    3.280   \n",
      "46        4.76       35.05       16.97     6.78    2.770   \n",
      "47        1.64       30.35       17.50    14.46    6.230   \n",
      "48        4.72       23.33       16.09    10.46    5.110   \n",
      "49        5.52       19.14        8.71     5.73    0.470   \n",
      "\n",
      "               img_FourC_ED             img_FourC_ES            img_PLAX_ED  \\\n",
      "0            1_FourC_ED.jpg           1_FourC_ES.jpg          1_PLAX_ED.jpg   \n",
      "1            2_FourC_ED.jpg           2_FourC_ES.jpg          2_PLAX_ED.jpg   \n",
      "2            3_FourC_ED.jpg           3_FourC_ES.jpg          3_PLAX_ED.jpg   \n",
      "3            4_FourC_ED.jpg           4_FourC_ES.jpg          4_PLAX_ED.jpg   \n",
      "4            5_FourC_ED.jpg           5_FourC_ES.jpg          5_PLAX_ED.jpg   \n",
      "5     6_FourC_ED59_(21).jpg    6_FourC_ES59_(30).jpg          6_PLAX_ED.jpg   \n",
      "6            7_FourC_ED.jpg           7_FourC_ES.jpg          7_PLAX_ED.jpg   \n",
      "7            8_FourC_ED.jpg           8_FourC_ES.jpg          8_PLAX_ED.jpg   \n",
      "8            9_FourC_ED.jpg           9_FourC_ES.jpg          9_PLAX_ED.jpg   \n",
      "9    10_FourC_ED67_(29).jpg   10_FourC_ES67_(39).jpg   10_PLAX_ED9_(31).jpg   \n",
      "10          11_FourC_ED.jpg          11_FourC_ES.jpg         11_PLAX_ED.jpg   \n",
      "11    12_FourC_ED27_(1).jpg   12_FourC_ES27_(10).jpg   12_PLAX_ED5_(18).jpg   \n",
      "12  13_FourC_ED39_(107).jpg  13_FourC_ES39_(136).jpg   13_PLAX_ED4_(54).jpg   \n",
      "13  14_FourC_ED114_(28).jpg  14_FourC_ES114_(38).jpg   14_PLAX_ED3_(28).jpg   \n",
      "14   15_FourC_ED32_(26).jpg   15_FourC_ES32_(35).jpg   15_PLAX_ED3_(22).jpg   \n",
      "15   16_FourC_ED39_(30).jpg   16_FourC_ES39_(41).jpg   16_PLAX_ED3_(30).jpg   \n",
      "16   17_FourC_ED52_(42).jpg   17_FourC_ES52_(59).jpg   17_PLAX_ED3_(41).jpg   \n",
      "17   18_FourC_ED67_(32).jpg   18_FourC_ES67_(43).jpg   18_PLAX_ED4_(31).jpg   \n",
      "18   19_FourC_ED88_(50).jpg   19_FourC_ES88_(69).jpg   19_PLAX_ED4_(46).jpg   \n",
      "19   20_FourC_ED50_(24).jpg   20_FourC_ES50_(33).jpg   20_PLAX_ED3_(21).jpg   \n",
      "20  21_FourC_ED115_(28).jpg  21_FourC_ES115_(37).jpg   21_PLAX_ED3_(25).jpg   \n",
      "21   22_FourC_ED37_(25).jpg   22_FourC_ES37_(33).jpg   22_PLAX_ED5_(24).jpg   \n",
      "22   23_FourC_ED52_(26).jpg   23_FourC_ES52_(34).jpg   23_PLAX_ED4_(24).jpg   \n",
      "23   24_FourC_ED91_(32).jpg   24_FourC_ES91_(41).jpg   24_PLAX_ED4_(32).jpg   \n",
      "24   25_FourC_ED36_(34).jpg   25_FourC_ES36_(44).jpg   25_PLAX_ED3_(32).jpg   \n",
      "25    26_FourC_ED31_(2).jpg   26_FourC_ES31_(10).jpg   26_PLAX_ED11_(1).jpg   \n",
      "26   27_FourC_ED65_(23).jpg   27_FourC_ES65_(34).jpg  27_PLAX_ED13_(25).jpg   \n",
      "27    28_FourC_ED46_(2).jpg   28_FourC_ES46_(19).jpg  28_PLAX_ED17_(49).jpg   \n",
      "28   29_FourC_ED37_(52).jpg   29_FourC_ES37_(70).jpg   29_PLAX_ED3_(63).jpg   \n",
      "29  30_FourC_ED105_(39).jpg  30_FourC_ES105_(46).jpg   30_PLAX_ED3_(22).jpg   \n",
      "30    31_FourC_ED68_(2).jpg   31_FourC_ES68_(12).jpg   31_PLAX_ED8_(23).jpg   \n",
      "31   32_FourC_ED42_(25).jpg   32_FourC_ES42_(35).jpg    32_PLAX_ED3_(1).jpg   \n",
      "32   33_FourC_ED80_(28).jpg   33_FourC_ES80_(38).jpg   33_PLAX_ED8_(50).jpg   \n",
      "33    34_FourC_ED86_(1).jpg   34_FourC_ES86_(11).jpg   34_PLAX_ED4_(26).jpg   \n",
      "34   35_FourC_ED38_(83).jpg  35_FourC_ES38_(110).jpg   35_PLAX_ED3_(47).jpg   \n",
      "35    36_FourC_ED37_(3).jpg   36_FourC_ES37_(12).jpg    36_PLAX_ED3_(1).jpg   \n",
      "36   37_FourC_ED69_(34).jpg   37_FourC_ES69_(42).jpg    37_PLAX_ED3_(3).jpg   \n",
      "37  38_FourC_ED127_(54).jpg  38_FourC_ES127_(34).jpg   38_PLAX_ED4_(33).jpg   \n",
      "38   39_FourC_ED42_(32).jpg   39_FourC_ES42_(42).jpg  39_PLAX_ED12_(32).jpg   \n",
      "39  40_FourC_ED100_(51).jpg  40_FourC_ES100_(60).jpg   40_PLAX_ED3_(26).jpg   \n",
      "40    41_FourC_ED58_(2).bmp   41_FourC_ES58_(12).bmp   41_PLAX_ED3_(33).jpg   \n",
      "41  42_FourC_ED116_(19).jpg  42_FourC_ES116_(29).jpg   42_PLAX_ED4_(19).jpg   \n",
      "42   43_FourC_ED47_(37).jpg   43_FourC_ES47_(47).jpg    43_PLAX_ED3_(1).jpg   \n",
      "43   44_FourC_ED36_(21).jpg   44_FourC_ES36_(29).jpg   44_PLAX_ED3_(21).jpg   \n",
      "44   45_FourC_ED76_(38).jpg   45_FourC_ES76_(48).jpg   45_PLAX_ED8_(37).jpg   \n",
      "45   46_FourC_ED39_(28).jpg   46_FourC_ES39_(35).jpg   46_PLAX_ED2_(26).jpg   \n",
      "46    47_FourC_ED45_(2).jpg   47_FourC_ES45_(15).jpg  47_PLAX_ED63_(33).jpg   \n",
      "47   48_FourC_ED47_(16).jpg   48_FourC_ES47_(23).jpg   48_PLAX_ED9_(13).jpg   \n",
      "48   49_FourC_ED57_(58).jpg   49_FourC_ES57_(66).jpg   49_PLAX_ED3_(81).jpg   \n",
      "49   50_FourC_ED42_(18).jpg   50_FourC_ES42_(25).jpg   50_PLAX_ED6_(17).jpg   \n",
      "\n",
      "              img_PLAX_ES            img_PSAX_AV_ED            img_PSAX_AV_ES  \\\n",
      "0           1_PLAX_ES.jpg          1_PSAX_AV_ED.jpg          1_PSAX_AV_ES.jpg   \n",
      "1           2_PLAX_ES.jpg          2_PSAX_AV_ED.jpg          2_PSAX_AV_ES.jpg   \n",
      "2           3_PLAX_ES.jpg          3_PSAX_AV_ED.jpg          3_PSAX_AV_ES.jpg   \n",
      "3           4_PLAX_ES.jpg          4_PSAX_AV_ED.jpg          4_PSAX_AV_ES.jpg   \n",
      "4           5_PLAX_ES.jpg          5_PSAX_AV_ED.jpg          5_PSAX_AV_ES.jpg   \n",
      "5           6_PLAX_ES.jpg          6_PSAV_AV_ED.jpg          6_PSAX_AV_ES.jpg   \n",
      "6           7_PLAX_ES.jpg          7_PSAV_AV_ED.jpg          7_PSAX_AV_ES.jpg   \n",
      "7           8_PLAX_ES.jpg          8_PSAX_AV_ED.jpg          8_PSAX_AV_ES.jpg   \n",
      "8           9_PLAX_ES.jpg          9_PSAX_AV_ED.jpg          9_PSAX_AV_ES.jpg   \n",
      "9    10_PLAX_ES9_(43).jpg  10_PSAX_AV_ED21_(33).jpg  10_PSAX_AV_ES21_(43).jpg   \n",
      "10         11_PLAX_ES.jpg         11_PSAX_AV_ED.jpg         11_PSAX_AV_ES.jpg   \n",
      "11   12_PLAX_ES5_(31).jpg  12_PSAX_AV_ED16_(22).jpg  12_PSAX_AV_ES16_(32).jpg   \n",
      "12   13_PLAX_ES4_(72).jpg   13_PSAV_AV_ED21_(1).jpg  13_PSAX_AV_ES21_(20).jpg   \n",
      "13   14_PLAX_ES3_(40).jpg  14_PSAX_AV_ED57_(26).jpg  14_PSAX_AV_ES57_(38).jpg   \n",
      "14   15_PLAX_ES3_(32).jpg  15_PSAX_AV_ED18_(22).jpg  15_PSAX_AV_ES18_(32).jpg   \n",
      "15   16_PLAX_ES3_(41).jpg   16_PSAX_AV_ED25_(2).jpg   16_PSAX_AV_ES25_(7).jpg   \n",
      "16   17_PLAX_ES3_(59).jpg  17_PSAX_AV_ED38_(15).jpg  17_PSAX_AV_ES38_(23).jpg   \n",
      "17   18_PLAX_ES4_(41).jpg  18_PSAV_AV_ED31_(32).jpg  18_PSAX_AV_ES31_(43).jpg   \n",
      "18   19_PLAX_ES4_(67).jpg   19_PSAX_AV_ED53_(3).jpg  19_PSAX_AV_ES53_(23).jpg   \n",
      "19   20_PLAX_ES3_(31).jpg  20_PSAX_AV_ED36_(11).jpg  20_PSAX_AV_ES36_(17).jpg   \n",
      "20   21_PLAX_ES3_(35).jpg  21_PSAX_AV_ED82_(29).jpg  21_PSAX_AV_ES82_(39).jpg   \n",
      "21   22_PLAX_ES5_(33).jpg  22_PSAX_AV_ED21_(25).jpg  22_PSAX_AV_ES21_(33).jpg   \n",
      "22   23_PLAX_ES4_(36).jpg  23_PSAX_AV_ED32_(14).jpg  23_PSAX_AV_ES32_(19).jpg   \n",
      "23   24_PLAX_ES4_(43).jpg  24_PSAX_AV_ED33_(31).jpg  24_PSAX_AV_ES33_(41).jpg   \n",
      "24   25_PLAX_ES3_(42).jpg  25_PSAX_AV_ED21_(32).jpg  25_PSAX_AV_ES21_(46).jpg   \n",
      "25  26_PLAX_ES11_(11).jpg  26_PSAX_AV_ED20_(25).jpg  26_PSAX_AV_ES20_(34).jpg   \n",
      "26  27_PLAX_ES13_(35).jpg  27_PSAX_AV_ED28_(28).jpg  27_PSAX_AV_ES28_(37).jpg   \n",
      "27  28_PLAX_ES17_(65).jpg   28_PSAX_AV_ED32_(2).jpg   28_PSAX_AV_ES32_(9).jpg   \n",
      "28   29_PLAX_ES3_(85).jpg   29_PSAX_AV_ED25_(3).jpg  29_PSAX_AV_ES25_(10).jpg   \n",
      "29   30_PLAX_ES3_(31).jpg  30_PSAX_AV_ED29_(23).jpg  30_PSAX_AV_ES29_(34).jpg   \n",
      "30   31_PLAX_ES8_(34).jpg  31_PSAX_AV_ED28_(30).jpg  31_PSAX_AV_ES28_(40).jpg   \n",
      "31   32_PLAX_ES3_(11).jpg  32_PSAX_AV_ED27_(26).jpg  32_PSAX_AV_ES27_(37).jpg   \n",
      "32   33_PLAX_ES8_(61).jpg  33_PSAX_AV_ED38_(28).jpg  33_PSAX_AV_ES38_(37).jpg   \n",
      "33   34_PLAX_ES4_(36).jpg  34_PSAX_AV_ED38_(24).jpg  34_PSAX_AV_ES38_(32).jpg   \n",
      "34   35_PLAX_ES3_(64).jpg  35_PSAX_AV_ED18_(46).jpg  35_PSAX_AV_ES18_(63).jpg   \n",
      "35   36_PLAX_ES3_(11).jpg   36_PSAX_AV_ED18_(2).jpg  36_PSAX_AV_ES18_(11).jpg   \n",
      "36   37_PLAX_ES3_(11).jpg  37_PSAX_AV_ED34_(50).jpg  37_PSAX_AV_ES34_(61).jpg   \n",
      "37   38_PLAX_ES4_(43).jpg  38_PSAX_AV_ED61_(29).jpg  38_PSAX_AV_ES61_(38).jpg   \n",
      "38  39_PLAX_ES12_(44).jpg  39_PSAX_AV_ED20_(31).jpg  39_PSAX_AV_ES20_(40).jpg   \n",
      "39   40_PLAX_ES3_(34).jpg  40_PSAX_AV_ED30_(24).jpg  40_PSAX_AV_ES30_(34).jpg   \n",
      "40   41_PLAX_ES3_(45).jpg   41_PSAX_AV_ED35_(8).bmp  41_PSAX_AV_ES35_(20).bmp   \n",
      "41   42_PLAX_ES4_(28).jpg   42_PSAX_AV_ED39_(1).jpg   42_PSAX_AV_ES39_(7).jpg   \n",
      "42   43_PLAX_ES3_(14).jpg  43_PSAX_AV_ED26_(35).jpg  43_PSAX_AV_ES26_(47).jpg   \n",
      "43   44_PLAX_ES3_(29).jpg  44_PSAX_AV_ED18_(22).jpg  44_PSAX_AV_ES18_(29).jpg   \n",
      "44   45_PLAX_ES8_(49).jpg  45_PSAX_AV_ED28_(37).jpg  45_PSAX_AV_ES28_(50).jpg   \n",
      "45   46_PLAX_ES2_(36).jpg  46_PSAX_AV_ED21_(30).jpg  46_PSAX_AV_ES21_(39).jpg   \n",
      "46  47_PLAX_ES63_(46).jpg  47_PSAX_AV_ED89_(34).jpg  47_PSAX_AV_ES89_(46).jpg   \n",
      "47   48_PLAX_ES9_(19).jpg  48_PSAX_AV_ED25_(19).jpg  48_PSAX_AV_ES25_(28).jpg   \n",
      "48   49_PLAX_ES3_(91).jpg  49_PSAX_AV_ED27_(30).jpg  49_PSAX_AV_ES27_(38).jpg   \n",
      "49   50_PLAX_ES6_(24).jpg   50_PSAX_AV_ED22_(2).jpg   50_PSAX_AV_ES22_(9).jpg   \n",
      "\n",
      "              img_PSAXbase_ED             img_PSAXbase_ES  \\\n",
      "0     1_PSAXbase_ED28_(1).jpg    1_PSAXbase_ES28_(13).jpg   \n",
      "1           2_PSAXbase_ED.jpg           2_PSAXbase_ES.jpg   \n",
      "2           3_PSAXbase_ED.jpg           3_PSAXbase_ES.jpg   \n",
      "3           4_PSAXbase_ED.jpg           4_PSAXbase_ES.jpg   \n",
      "4           5_PSAXbase_ED.jpg           5_PSAXbase_ES.jpg   \n",
      "5           6_PSAXbase_ED.jpg           6_PSAXbase_ES.jpg   \n",
      "6           7_PSAXbase_ED.jpg           7_PSAXbase_ES.jpg   \n",
      "7           8_PSAXbase_ED.jpg           8_PSAXbase_ES.jpg   \n",
      "8           9_PSAXbase_ED.jpg           9_PSAXbase_ES.jpg   \n",
      "9   10_PSAXbase_ED39_(31).jpg   10_PSAXbase_ES39_(42).jpg   \n",
      "10         11_PSAXbase_ED.jpg          11_PSAXbase_ES.jpg   \n",
      "11   12_PSAXbase_ED24_(1).jpg   12_PSAXbase_ES24_(10).jpg   \n",
      "12  13_PSAXbase_ED33_(55).jpg   13_PSAXbase_ES33_(72).jpg   \n",
      "13  14_PSAXbase_ED64_(28).jpg   14_PSAXbase_ES64_(39).jpg   \n",
      "14  15_PSAXbase_ED25_(20).jpg   15_PSAXbase_ES25_(31).jpg   \n",
      "15  16_PSAXbase_ED29_(30).jpg   16_PSAXbase_ES29_(42).jpg   \n",
      "16   17_PSAXbase_ED42_(2).jpg   17_PSAXbase_ES42_(19).jpg   \n",
      "17   18_PSAXbase_ED24_(2).jpg   18_PSAXbase_ES24_(14).jpg   \n",
      "18  19_PSAXbase_ED56_(48).jpg   19_PSAXbase_ES56_(69).jpg   \n",
      "19  20_PSAXbase_ED38_(20).jpg   20_PSAXbase_ES38_(29).jpg   \n",
      "20  21_PSAXbase_ED99_(31).jpg   21_PSAXbase_ES99_(39).jpg   \n",
      "21  22_PSAXbase_ED30_(24).jpg   22_PSAXbase_ES30_(31).jpg   \n",
      "22  23_PSAXbase_ED28_(25).jpg   23_PSAXbase_ES28_(35).jpg   \n",
      "23  24_PSAXbase_ED46_(30).jpg   24_PSAXbase_ES46_(40).jpg   \n",
      "24  25_PSAXbase_ED28_(33).jpg   25_PSAXbase_ES28_(44).jpg   \n",
      "25  26_PSAXbase_ED23_(24).jpg   26_PSAXbase_ES23_(34).jpg   \n",
      "26  27_PSAXbase_ED36_(23).jpg   27_PSAXbase_ES36_(35).jpg   \n",
      "27  28_PSAXbase_ED40_(47).jpg   28_PSAXbase_ES40_(66).jpg   \n",
      "28  29_PSAXbase_ED30_(57).jpg   29_PSAXbase_ES30_(79).jpg   \n",
      "29  30_PSAXbase_ED24_(23).jpg   30_PSAXbase_ES24_(32).jpg   \n",
      "30  31_PSAXbase_ED41_(29).jpg   31_PSAXbase_ES41_(39).jpg   \n",
      "31  32_PSAXbase_ED36_(28).jpg   32_PSAXbase_ES36_(37).jpg   \n",
      "32  33_PSAXbase_ED50_(27).jpg   33_PSAXbase_ES50_(39).jpg   \n",
      "33  34_PSAXbase_ED39_(25).jpg   34_PSAXbase_ES39_(33).jpg   \n",
      "34  35_PSAXbase_ED27_(47).jpg   35_PSAXbase_ES27_(63).jpg   \n",
      "35  36_PSAXbase_ED27_(21).jpg   36_PSAXbase_ES27_(30).jpg   \n",
      "36  37_PSAXbase_ED49_(25).jpg   37_PSAXbase_ES49_(35).jpg   \n",
      "37  38_PSAXbase_ED86_(29).jpg   38_PSAXbase_ES86_(39).jpg   \n",
      "38  39_PSAXbase_ED30_(41).jpg   39_PSAXbase_ES30_(41).jpg   \n",
      "39  40_PSAXbase_ED40_(22).jpg   40_PSAXbase_ES40_(29).jpg   \n",
      "40  41_PSAXbase_ED38_(34).bmp   41_PSAXbase_ES38_(46).bmp   \n",
      "41  42_PSAXbase_ED62_(38).jpg   42_PSAXbase_ES62_(45).jpg   \n",
      "42  43_PSAXbase_ED37_(37).jpg   43_PSAXbase_ES37_(49).jpg   \n",
      "43  44_PSAXbase_ED27_(21).jpg   44_PSAXbase_ES27_(30).jpg   \n",
      "44  45_PSAXbase_ED55_(37).jpg   45_PSAXbase_ES55_(51).jpg   \n",
      "45  46_PSAXbase_ED32_(26).jpg   46_PSAXbase_ES32_(36).jpg   \n",
      "46  47_PSAXbase_ED105_(1).jpg  47_PSAXbase_ES105_(11).jpg   \n",
      "47   48_PSAXbase_ED23_(1).jpg   48_PSAXbase_ES23_(10).jpg   \n",
      "48  49_PSAXbase_ED39_(29).jpg   49_PSAXbase_ES39_(40).jpg   \n",
      "49  50_PSAXbase_ED29_(17).jpg   50_PSAXbase_ES29_(23).jpg   \n",
      "\n",
      "               img_PSAXdistal_ED             img_PSAXdistal_ES  \\\n",
      "0            1_PSAXdistal_ED.jpg           1_PSAXdistal_ES.jpg   \n",
      "1            2_PSAXdistal_ED.jpg           2_PSAXdistal_ES.jpg   \n",
      "2            3_PSAXdistal_ED.jpg           3_PSAXdistal_ES.jpg   \n",
      "3            4_PSAXdistal_ED.jpg           4_PSAXdistal_ES.jpg   \n",
      "4            5_PSAXdistal_ED.jpg           5_PSAXdistal_ES.jpg   \n",
      "5            6_PSAXdistal_ED.jpg           6_PSAXdistal_ES.jpg   \n",
      "6            7_PSAXdistal_ED.jpg           7_PSAXdistal_ES.jpg   \n",
      "7            8_PSAXdistal_ED.jpg           8_PSAXdistal_ES.jpg   \n",
      "8            9_PSAXdistal_ED.jpg           9_PSAXdistal_ES.jpg   \n",
      "9     10_PSAXdistal_ED33_(1).jpg   10_PSAXdistal_ES33_(12).jpg   \n",
      "10          11_PSAXdistal_ED.jpg          11_PSAXdistal_ES.jpg   \n",
      "11    12_PSAXdistal_ED26_(1).jpg   12_PSAXdistal_ES26_(10).jpg   \n",
      "12   13_PSAXdistal_ED36_(53).jpg   13_PSAXdistal_ES36_(73).jpg   \n",
      "13    14_PSAXdistal_ED70_(1).jpg   14_PSAXdistal_ES70_(14).jpg   \n",
      "14   15_PSAXdistal_ED30_(20).jpg   15_PSAXdistal_ES30_(29).jpg   \n",
      "15    16_PSAXdistal_ED33_(1).jpg   16_PSAXdistal_ES33_(12).jpg   \n",
      "16   17_PSAXdistal_ED48_(38).jpg   17_PSAXdistal_ES48_(54).jpg   \n",
      "17   18_PSAXdistal_ED29_(30).jpg   18_PSAXdistal_ES29_(42).jpg   \n",
      "18   19_PSAXdistal_ED65_(48).jpg   19_PSAXdistal_ES65_(69).jpg   \n",
      "19   20_PSAXdistal_ED42_(22).jpg   20_PSAXdistal_ES42_(30).jpg   \n",
      "20   21_PSAXdistal_ED105_(1).jpg  21_PSAXdistal_ES105_(10).jpg   \n",
      "21    22_PSAXdistal_ED33_(1).jpg   22_PSAXdistal_ES33_(10).jpg   \n",
      "22   23_PSAXdistal_ED25_(24).jpg   23_PSAXdistal_ES25_(34).jpg   \n",
      "23    24_PSAXdistal_ED53_(1).jpg   24_PSAXdistal_ES53_(13).jpg   \n",
      "24   25_PSAXdistal_ED33_(32).jpg   25_PSAXdistal_ES33_(45).jpg   \n",
      "25    26_PSAXdistal_ED27_(1).jpg    26_PSAXdistal_ES27_(9).jpg   \n",
      "26    27_PSAXdistal_ED42_(1).jpg    27_PSAXdistal_ES42_(9).jpg   \n",
      "27   28_PSAXdistal_ED44_(53).jpg   28_PSAXdistal_ES44_(73).jpg   \n",
      "28   29_PSAXdistal_ED32_(59).jpg   29_PSAXdistal_ES32_(81).jpg   \n",
      "29   30_PSAXdistal_ED28_(22).jpg   30_PSAXdistal_ES28_(31).jpg   \n",
      "30    31_PSAXdistal_ED48_(1).jpg   31_PSAXdistal_ES48_(13).jpg   \n",
      "31    32_PSAXdistal_ED40_(1).jpg   32_PSAXdistal_ES40_(10).jpg   \n",
      "32   33_PSAXdistal_ED55_(51).jpg   33_PSAXdistal_ES55_(60).jpg   \n",
      "33   34_PSAXdistal_ED43_(25).jpg   34_PSAXdistal_ES43_(34).jpg   \n",
      "34    35_PSAXdistal_ED30_(1).jpg   35_PSAXdistal_ES30_(18).jpg   \n",
      "35   36_PSAXdistal_ED32_(21).jpg   36_PSAXdistal_ES32_(30).jpg   \n",
      "36   37_PSAXdistal_ED54_(24).jpg   37_PSAXdistal_ES54_(33).jpg   \n",
      "37    38_PSAXdistal_ED93_(1).jpg   38_PSAXdistal_ES93_(12).jpg   \n",
      "38   39_PSAXdistal_ED34_(34).jpg   39_PSAXdistal_ES34_(44).jpg   \n",
      "39   40_PSAXdistal_ED45_(25).jpg   40_PSAXdistal_ES45_(33).jpg   \n",
      "40   41_PSAXdistal_ED44_(32).bmp   41_PSAXdistal_ES44_(43).bmp   \n",
      "41    42_PSAXdistal_ED66_(1).jpg    42_PSAXdistal_ES66_(8).jpg   \n",
      "42   43_PSAXdistal_ED43_(34).jpg   43_PSAXdistal_ES43_(50).jpg   \n",
      "43   44_PSAXdistal_ED30_(21).jpg   44_PSAXdistal_ES30_(30).jpg   \n",
      "44   45_PSAXdistal_ED58_(36).jpg   45_PSAXdistal_ES58_(48).jpg   \n",
      "45    46_PSAXdistal_ED34_(1).jpg   46_PSAXdistal_ES34_(11).jpg   \n",
      "46  47_PSAXdistal_ED108_(33).jpg  47_PSAXdistal_ES108_(44).jpg   \n",
      "47    48_PSAXdistal_ED19_(1).jpg   48_PSAXdistal_ES19_(11).jpg   \n",
      "48   49_PSAXdistal_ED42_(28).jpg   49_PSAXdistal_ES42_(36).jpg   \n",
      "49   50_PSAXdistal_ED34_(17).jpg   50_PSAXdistal_ES34_(25).jpg   \n",
      "\n",
      "              img_PSAXmid_ED             img_PSAXmid_ES  \\\n",
      "0           1_PSAXmid_ED.jpg          1_PSAXmid_ES2.jpg   \n",
      "1           2_PSAXmid_ED.jpg           2_PSAXmid_ES.jpg   \n",
      "2           3_PSAXmid_ED.jpg           3_PSAXmid_ES.jpg   \n",
      "3           4_PSAXmid_ED.jpg           4_PSAXmid_ES.jpg   \n",
      "4           5_PSAXmid_ED.jpg           5_PSAXmid_ES.jpg   \n",
      "5           6_PSAXmid_ED.jpg           6_PSAXmid_ES.jpg   \n",
      "6           7_PSAXmid_ED.jpg           7_PSAXmid_ES.jpg   \n",
      "7           8_PSAXmid_ED.jpg           8_PSAXmid_ES.jpg   \n",
      "8           9_PSAXmid_ED.jpg           9_PSAXmid_ES.jpg   \n",
      "9   10_PSAXmid_ED38_(30).jpg   10_PSAXmid_ES38_(41).jpg   \n",
      "10         11_PSAXmid_ED.jpg          11_PSAXmid_ES.jpg   \n",
      "11   12_PSAXmid_ED25_(1).jpg   12_PSAXmid_ES25_(11).jpg   \n",
      "12  13_PSAXmid_ED35_(54).jpg   13_PSAXmid_ES35_(75).jpg   \n",
      "13  14_PSAXmid_ED67_(27).jpg   14_PSAXmid_ES67_(39).jpg   \n",
      "14  15_PSAXmid_ED29_(22).jpg   15_PSAXmid_ES29_(30).jpg   \n",
      "15  16_PSAXmid_ED32_(30).jpg   16_PSAXmid_ES32_(42).jpg   \n",
      "16  17_PSAXmid_ED45_(38).jpg   17_PSAXmid_ES45_(56).jpg   \n",
      "17  18_PSAXmid_ED27_(31).jpg   18_PSAXmid_ES27_(45).jpg   \n",
      "18  19_PSAXmid_ED60_(51).jpg   19_PSAXmid_ES60_(72).jpg   \n",
      "19  20_PSAXmid_ED41_(21).jpg   20_PSAXmid_ES41_(32).jpg   \n",
      "20  21_PSAXmid_ED104_(1).jpg  21_PSAXmid_ES104_(12).jpg   \n",
      "21   22_PSAXmid_ED32_(2).jpg   22_PSAXmid_ES32_(10).jpg   \n",
      "22  23_PSAXmid_ED26_(23).jpg   23_PSAXmid_ES26_(35).jpg   \n",
      "23  24_PSAXmid_ED50_(28).jpg   24_PSAXmid_ES50_(40).jpg   \n",
      "24  25_PSAXmid_ED30_(33).jpg   25_PSAXmid_ES30_(44).jpg   \n",
      "25  26_PSAXmid_ED24_(25).jpg   26_PSAXmid_ES24_(34).jpg   \n",
      "26  27_PSAXmid_ED39_(23).jpg   27_PSAXmid_ES39_(37).jpg   \n",
      "27  28_PSAXmid_ED42_(56).jpg   28_PSAXmid_ES42_(74).jpg   \n",
      "28  29_PSAXmid_ED31_(55).jpg   29_PSAXmid_ES31_(75).jpg   \n",
      "29  30_PSAXmid_ED27_(22).jpg   30_PSAXmid_ES27_(31).jpg   \n",
      "30   31_PSAXmid_ED45_(1).jpg   31_PSAXmid_ES45_(11).jpg   \n",
      "31   32_PSAXmid_ED39_(1).jpg   32_PSAXmid_ES39_(11).jpg   \n",
      "32  33_PSAXmid_ED53_(25).jpg   33_PSAXmid_ES53_(36).jpg   \n",
      "33  34_PSAXmid_ED42_(25).jpg   34_PSAXmid_ES42_(33).jpg   \n",
      "34  35_PSAXmid_ED31_(41).jpg   35_PSAXmid_ES31_(60).jpg   \n",
      "35   36_PSAXmid_ED31_(1).jpg   36_PSAXmid_ES31_(11).jpg   \n",
      "36  37_PSAXmid_ED50_(28).jpg   37_PSAXmid_ES50_(39).jpg   \n",
      "37  38_PSAXmid_ED89_(28).jpg   38_PSAXmid_ES89_(39).jpg   \n",
      "38  39_PSAXmid_ED32_(59).jpg   39_PSAXmid_ES32_(41).jpg   \n",
      "39  40_PSAXmid_ED44_(24).jpg   40_PSAXmid_ES44_(33).jpg   \n",
      "40  41_PSAXmid_ED42_(35).bmp   41_PSAXmid_ES42_(47).bmp   \n",
      "41  42_PSAXmid_ED65_(19).jpg   42_PSAXmid_ES65_(28).jpg   \n",
      "42  43_PSAXmid_ED40_(35).jpg   43_PSAXmid_ES40_(50).jpg   \n",
      "43  44_PSAXmid_ED29_(19).jpg   44_PSAXmid_ES29_(30).jpg   \n",
      "44  45_PSAXmid_ED57_(37).jpg   45_PSAXmid_ES57_(51).jpg   \n",
      "45  46_PSAXmid_ED33_(24).jpg   46_PSAXmid_ES33_(35).jpg   \n",
      "46  47_PSAXmid_ED106_(1).jpg  47_PSAXmid_ES106_(13).jpg   \n",
      "47  48_PSAXmid_ED21_(27).jpg   48_PSAXmid_ES21_(34).jpg   \n",
      "48  49_PSAXmid_ED41_(76).jpg   49_PSAXmid_ES41_(86).jpg   \n",
      "49  50_PSAXmid_ED31_(17).jpg   50_PSAXmid_ES31_(25).jpg   \n",
      "\n",
      "                img_Rinflow_ED             img_Rinflow_ES  \\\n",
      "0             1_Rinflow_ED.jpg          1_RVinflow_ES.jpg   \n",
      "1            2_RVinflow_ED.jpg          2_RVinflow_ES.jpg   \n",
      "2            3_RVinflow_ED.jpg          3_RVinflow_ES.jpg   \n",
      "3            4_RVinflow_ED.jpg          4_RVinflow_ES.jpg   \n",
      "4            5_RVinflow_ED.jpg          5_RVinflow_ES.jpg   \n",
      "5            6_RVinflow_ED.jpg          6_RVinflow_ES.bmp   \n",
      "6            7_RVinflow_ED.jpg          7_RVinflow_ES.jpg   \n",
      "7            8_RVinflow_ED.jpg          8_RVinflow_ES.jpg   \n",
      "8            9_RVinflow_ED.jpg          9_RVinflow_ES.jpg   \n",
      "9    10_RVinflow_ED14_(32).jpg  10_RVinflow_ES14_(40).jpg   \n",
      "10          11_RVinflow_ED.jpg         11_RVinflow_ES.jpg   \n",
      "11   12_RVinflow_ED10_(22).jpg  12_RVinflow_ES10_(34).jpg   \n",
      "12    13_RVinflow_ED14_(2).jpg  13_RVinflow_ES14_(22).jpg   \n",
      "13   14_RVinflow_ED36_(26).jpg  14_RVinflow_ES36_(38).jpg   \n",
      "14   15_RVinflow_ED10_(21).jpg  15_RVinflow_ES10_(31).jpg   \n",
      "15    16_RVinflow_ED15_(2).jpg  16_RVinflow_ES15_(14).jpg   \n",
      "16    17_RVinflow_ED23_(4).jpg  17_RVinflow_ES23_(21).jpg   \n",
      "17   18_RVinflow_ED19_(32).jpg  18_RVinflow_ES19_(43).jpg   \n",
      "18   19_RVinflow_ED30_(49).jpg  19_RVinflow_ES30_(68).jpg   \n",
      "19   20_RVinflow_ED16_(24).jpg  20_RVinflow_ES16_(34).jpg   \n",
      "20    21_RVinflow_ED49_(1).jpg  21_RVinflow_ES49_(12).jpg   \n",
      "21    22_RVinflow_ED13_(2).jpg  22_RVinflow_ES13_(10).jpg   \n",
      "22   23_RVinflow_ED14_(26).jpg  23_RVinflow_ES14_(36).jpg   \n",
      "23    24_RVinflow_ED21_(2).jpg  24_RVinflow_ES21_(11).jpg   \n",
      "24   25_RVinflow_ED14_(32).jpg  25_RVinflow_ES14_(43).jpg   \n",
      "25   26_RVinflow_ED12_(25).jpg  26_RVinflow_ES12_(35).jpg   \n",
      "26   27_RVinflow_ED15_(24).jpg  27_RVinflow_ES15_(34).jpg   \n",
      "27    28_RVinflow_ED22_(1).jpg   28_RVinflow_ES22_(8).jpg   \n",
      "28  29_RVinflow_ED12_(122).jpg  29_RVinflow_ES12_(88).jpg   \n",
      "29   30_RVinflow_ED110_(1).jpg  30_RVinflow_ES110_(9).jpg   \n",
      "30   31_RVinflow_ED17_(28).jpg  31_RVinflow_ES17_(38).jpg   \n",
      "31   32_RVinflow_ED15_(24).jpg  32_RVinflow_ES15_(34).jpg   \n",
      "32    33_RVinflow_ED26_(3).jpg  33_RVinflow_ES26_(12).jpg   \n",
      "33   34_RVinflow_ED21_(20).jpg  34_RVinflow_ES21_(30).jpg   \n",
      "34   35_RVinflow_ED10_(52).jpg  35_RVinflow_ES10_(68).jpg   \n",
      "35    36_RVinflow_ED11_(3).jpg  36_RVinflow_ES11_(12).jpg   \n",
      "36   37_RVinflow_ED17_(33).jpg  37_RVinflow_ES17_(46).jpg   \n",
      "37   38_RVinflow_ED45_(29).jpg  38_RVinflow_ES45_(39).jpg   \n",
      "38   39_RVinflow_ED13_(32).jpg  39_RVinflow_ES13_(43).jpg   \n",
      "39   40_RVinflow_ED17_(28).jpg  40_RVinflow_ES17_(35).jpg   \n",
      "40    41_RVinflow_ED15_(3).jpg  41_RVinflow_ES15_(13).jpg   \n",
      "41   42_RVinflow_ED21_(38).jpg  42_RVinflow_ES21_(45).jpg   \n",
      "42   43_RVinflow_ED18_(22).jpg  43_RVinflow_ES18_(28).jpg   \n",
      "43   44_RVinflow_ED11_(21).jpg  44_RVinflow_ES11_(30).jpg   \n",
      "44   45_RVinflow_ED24_(39).jpg  45_RVinflow_ES24_(48).jpg   \n",
      "45   46_RVinflow_ED14_(27).jpg  46_RVinflow_ES14_(36).jpg   \n",
      "46    47_RVinflow_ED83_(1).jpg  47_RVinflow_ES83_(12).jpg   \n",
      "47   48_RVinflow_ED12_(25).jpg  48_RVinflow_ES12_(35).jpg   \n",
      "48    49_RVinflow_ED15_(2).jpg   49_RVinflow_ES15_(8).jpg   \n",
      "49    50_RVinflow_ED12_(2).jpg   50_RVinflow_ES12_(9).jpg   \n",
      "\n",
      "               img_SubC_ED             img_SubC_ES  \n",
      "0            1_SubC_ED.jpg           1_SubC_ES.jpg  \n",
      "1            2_SubC_ED.jpg           2_SubC_ES.jpg  \n",
      "2            3_SubC_ED.jpg           3_SubC_ES.jpg  \n",
      "3            4_SubC_ED.jpg          4_SubC_ES8.jpg  \n",
      "4            5_SubC_ED.jpg           5_SubC_ES.jpg  \n",
      "5     6_SubC_ED97_(22).jpg    6_SubC_ES97_(31).jpg  \n",
      "6            7_SubC_ED.jpg           7_SubC_ES.jpg  \n",
      "7            8_SubC_ED.jpg           8_SubC_ES.jpg  \n",
      "8            9_SubC_ED.jpg           9_SubC_ES.jpg  \n",
      "9   10_SubC_ED105_(32).jpg  10_SubC_ES105_(43).jpg  \n",
      "10          11_SubC_ED.jpg          11_SubC_ES.jpg  \n",
      "11   12_SubC_ED53_(45).jpg   12_SubC_ES53_(56).jpg  \n",
      "12   13_SubC_ED74_(20).jpg   13_SubC_ES74_(29).jpg  \n",
      "13   14_SubC_ED215_(2).jpg  14_SubC_ES215_(13).jpg  \n",
      "14   15_SubC_ED51_(24).jpg   15_SubC_ES51_(33).jpg  \n",
      "15    16_SubC_ED72_(2).jpg   16_SubC_ES72_(11).jpg  \n",
      "16   17_SubC_ED86_(40).jpg   17_SubC_ES86_(59).jpg  \n",
      "17    18_SubC_ED85_(2).jpg   18_SubC_ES85_(12).jpg  \n",
      "18   19_SubC_ED167_(2).jpg  19_SubC_ES167_(17).jpg  \n",
      "19  20_SubC_ED100_(24).jpg  20_SubC_ES100_(34).jpg  \n",
      "20  21_SubC_ED159_(30).jpg  21_SubC_ES159_(38).jpg  \n",
      "21    22_SubC_ED58_(1).jpg   22_SubC_ES58_(13).jpg  \n",
      "22   23_SubC_ED71_(23).jpg   23_SubC_ES71_(34).jpg  \n",
      "23   24_SubC_ED141_(1).jpg  24_SubC_ES141_(13).jpg  \n",
      "24    25_SubC_ED96_(2).jpg   25_SubC_ES96_(12).jpg  \n",
      "25    26_SubC_ED57_(2).jpg   26_SubC_ES57_(10).jpg  \n",
      "26   27_SubC_ED98_(23).jpg   27_SubC_ES98_(33).jpg  \n",
      "27  28_SubC_ED87_(128).jpg  28_SubC_ES87_(102).jpg  \n",
      "28   29_SubC_ED79_(26).jpg   29_SubC_ES79_(47).jpg  \n",
      "29    30_SubC_ED87_(1).jpg   30_SubC_ES87_(12).jpg  \n",
      "30  31_SubC_ED105_(29).jpg  31_SubC_ES105_(40).jpg  \n",
      "31    32_SubC_ED82_(2).jpg    32_SubC_ES82_(6).jpg  \n",
      "32  33_SubC_ED119_(27).jpg  33_SubC_ES119_(36).jpg  \n",
      "33  34_SubC_ED105_(28).jpg  34_SubC_ES105_(38).jpg  \n",
      "34   35_SubC_ED60_(34).jpg   35_SubC_ES60_(47).jpg  \n",
      "35   36_SubC_ED71_(31).jpg   36_SubC_ES71_(41).jpg  \n",
      "36  37_SubC_ED111_(32).jpg  37_SubC_ES111_(43).jpg  \n",
      "37  38_SubC_ED217_(30).jpg  38_SubC_ES217_(40).jpg  \n",
      "38    39_SubC_ED69_(3).jpg   39_SubC_ES69_(13).jpg  \n",
      "39    40_SubC_ED91_(1).jpg   40_SubC_ES91_(10).jpg  \n",
      "40  41_SubC_ED106_(34).bmp  41_SubC_ES106_(45).bmp  \n",
      "41  42_SubC_ED174_(20).jpg  42_SubC_ES174_(20).jpg  \n",
      "42    43_SubC_ED85_(2).jpg   43_SubC_ES85_(12).jpg  \n",
      "43   44_SubC_ED80_(19).jpg   44_SubC_ES80_(28).jpg  \n",
      "44  45_SubC_ED132_(39).jpg  45_SubC_ES132_(47).jpg  \n",
      "45    46_SubC_ED86_(1).jpg   46_SubC_ES86_(12).jpg  \n",
      "46    47_SubC_ED3_(32).jpg    47_SubC_ES3_(44).jpg  \n",
      "47    48_SubC_ED62_(1).jpg   48_SubC_ES62_(10).jpg  \n",
      "48   49_SubC_ED84_(32).jpg   49_SubC_ES84_(39).jpg  \n",
      "49    50_SubC_ED67_(1).jpg    50_SubC_ES67_(9).jpg  \n"
     ]
    }
   ],
   "source": [
    "# Display the first five rows of the data\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FourC = pd.concat([df[\"FourC_ED\"], df[\"FourC_ES\"]])\n",
    "PLAX =pd.concat([df[\"PLAX_ED\"], df[\"PLAX_ES\"]])\n",
    "PSAX_AV =pd.concat([df[\"PSAX_AV_ED\"], df[\"PSAX_AV_ES\"]])\n",
    "PSAXbase =pd.concat([df[\"PSAXbase_ED\"], df[\"PSAXbase_ES\"]])\n",
    "PSAXdistal =pd.concat([df[\"PSAXdistal_ED\"], df[\"PSAXdistal_ES\"]])\n",
    "PSAXmid =pd.concat([df[\"PSAXmid_ED\"], df[\"PSAXmid_ES\"]])\n",
    "Rinflow =pd.concat([df[\"Rinflow_ED\"], df[\"Rinflow_ES\"]])\n",
    "SubC =pd.concat([df[\"SubC_ED\"], df[\"SubC_ES\"]])\n",
    "\n",
    "\n",
    "img_FourC = pd.concat([df[\"img_FourC_ED\"], df[\"img_FourC_ES\"]])\n",
    "img_PLAXD =pd.concat([df[\"img_PLAX_ED\"], df[\"img_PLAX_ES\"]])\n",
    "img_PSAX_AV =pd.concat([df[\"img_PSAX_AV_ED\"], df[\"img_PSAX_AV_ES\"]])\n",
    "img_PSAXbase =pd.concat([df[\"img_PSAXbase_ED\"], df[\"img_PSAXbase_ES\"]])\n",
    "img_PSAXdistal =pd.concat([df[\"img_PSAXdistal_ED\"], df[\"img_PSAXdistal_ES\"]])\n",
    "img_PSAXmid =pd.concat([df[\"img_PSAXmid_ED\"], df[\"img_PSAXmid_ES\"]])\n",
    "img_Rinflow =pd.concat([df[\"img_Rinflow_ED\"], df[\"img_Rinflow_ES\"]])\n",
    "img_SubC =pd.concat([df[\"img_SubC_ED\"], df[\"img_SubC_ED\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_excel('RV project patient list.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RV_Study_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age_at_earliest_study</th>\n",
       "      <th>CMR_RVEDV</th>\n",
       "      <th>CMR_RVESV</th>\n",
       "      <th>CMR_RVEF</th>\n",
       "      <th>ED</th>\n",
       "      <th>ES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>119</td>\n",
       "      <td>52</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>114</td>\n",
       "      <td>40</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>110</td>\n",
       "      <td>43</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>177</td>\n",
       "      <td>67</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>212</td>\n",
       "      <td>110</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RV_Study_ID  Gender  Age_at_earliest_study  CMR_RVEDV  CMR_RVESV  CMR_RVEF  \\\n",
       "0            1       0                     56        119         52        56   \n",
       "1            2       0                     53        114         40        65   \n",
       "2            3       0                     42        110         43        61   \n",
       "3            4       1                     61        177         67        62   \n",
       "4            5       1                     27        212        110        48   \n",
       "\n",
       "   ED  ES  \n",
       "0   0   1  \n",
       "1   0   1  \n",
       "2   0   1  \n",
       "3   0   1  \n",
       "4   0   1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "RVEDV = df1[\"CMR_RVEDV\"]\n",
    "RVESV = df1[\"CMR_RVESV\"]\n",
    "RVEF  = df1[\"CMR_RVEF\"]\n",
    "Phase = pd.concat([df1[\"ED\"], df1[\"ES\"]])\n",
    "Gender= pd.concat([df1[\"Gender\"], df1[\"Gender\"]])\n",
    "Age = pd.concat([df1[\"Age_at_earliest_study\"], df1[\"Age_at_earliest_study\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_FourC_train, img_FourC_test = img_FourC[10:-10].tolist(), img_FourC[:10].tolist() + img_FourC[50:60].tolist() \n",
    "\n",
    "FourC_train, FourC_test =  FourC[10:-10].tolist(), FourC[:10].tolist() + FourC[50:60].tolist() \n",
    "PLAX_train, PLAX_test = PLAX[10:-10].tolist(), PLAX[:10].tolist() + PLAX[50:60].tolist() \n",
    "PSAX_AV_train, PSAX_AV_test = PSAX_AV[10:-10].tolist(), PSAX_AV[:10].tolist() + PSAX_AV[50:60].tolist() \n",
    "PSAXbase_train, PSAXbase_test = PSAXbase[10:-10].tolist(), PSAXbase[:10].tolist() + PSAXbase[50:60].tolist() \n",
    "PSAXdistal_train, PSAXdistal_test = PSAXdistal[10:-10].tolist(), PSAXdistal[:10].tolist() + PSAXdistal[50:60].tolist() \n",
    "PSAXmid_train, PSAXmid_test= PSAXmid[10:-10].tolist(), PSAXmid[:10].tolist() + PSAXmid[50:60].tolist() \n",
    "Rinflow_train, Rinflow_test = Rinflow[10:-10].tolist(), Rinflow[:10].tolist() + Rinflow[50:60].tolist() \n",
    "SubC_train, SubC_test = SubC[10:-10].tolist(), SubC[:10].tolist() + SubC[50:60].tolist() \n",
    "Gender_train, Gender_test  = Gender[10:-10].tolist(), Gender[:10].tolist() + Gender[50:60].tolist() \n",
    "Age_train, Age_test  = Age[10:-10].tolist(), Age[:10].tolist() + Age[50:60].tolist() \n",
    "Phase_train, Phase_test  = Phase[10:-10].tolist(), Phase[:10].tolist() + Phase[50:60].tolist() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for i in range(len(FourC_train)):\n",
    "    X_train.append(np.array([Phase_train[i], Gender_train[i], Age_train[i], FourC_train[i], PLAX_train[i], PSAX_AV_train[i], PSAXbase_train[i], PSAXdistal_train[i], PSAXmid_train[i], Rinflow_train[i], SubC_train[i]]))\n",
    "    \n",
    "X_train = np.array(X_train, dtype='float32')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for i in range(len(FourC_test)):\n",
    "    X_test.append(np.array([Phase_test[i], Gender_test[i], Age_test[i], FourC_test[i], PLAX_test[i], PSAX_AV_test[i], PSAXbase_test[i], PSAXdistal_test[i], PSAXmid_test[i], Rinflow_test[i], SubC_test[i]]))\n",
    "    \n",
    "X_test = np.array(X_test, dtype='float32')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "RV_vol = pd.concat([df1[\"CMR_RVEDV\"], df1[\"CMR_RVESV\"]])\n",
    "\n",
    "RV_vol_train, RV_vol_test =  RV_vol[10:-10].tolist(), RV_vol[:10].tolist() + RV_vol[50:60].tolist() \n",
    "\n",
    "y_train = np.array(RV_vol_train, dtype='float32')\n",
    "\n",
    "y_test = np.array(RV_vol_test, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([119., 114., 110., 177., 212.,  97., 126., 188., 188., 124.,  52.,\n",
       "        40.,  43.,  67., 110.,  46.,  66., 100.,  88.,  46.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created scaler\n",
    "scaler = StandardScaler()\n",
    "# fit scaler on training dataset\n",
    "scaler.fit(y_train.reshape(-1, 1))\n",
    "# transform training dataset\n",
    "y_train = scaler.transform(y_train.reshape(-1, 1))\n",
    "# # transform test dataset\n",
    "# y_test = scaler.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 1, 11)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.  , 65.  , 16.8 , 14.01, 26.62,  9.44,  1.8 ,  5.77,\n",
       "        25.06, 10.21]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tab_transformer_pytorch import FTTransformer\n",
    "\n",
    "model = FTTransformer(\n",
    "    categories = (2, 2),      # tuple containing the number of unique values within each category\n",
    "    num_continuous = 9,                # number of continuous values\n",
    "    dim = 16,                           # dimension, paper set at 32\n",
    "    dim_out = 1,                        # binary prediction, but could be anything\n",
    "    depth = 3,                          # depth, paper recommended 6\n",
    "    heads = 24,                          # heads, paper recommends 8\n",
    "    attn_dropout = 0.3,                 # post-attention dropout\n",
    "    ff_dropout = 0.3                    # feed forward dropout\n",
    ")\n",
    "\n",
    "# cont_mean_std = torch.randn(11, 2)\n",
    "# x_categ = torch.randint(0, 5, (1, 5))     # category values, from 0 - max number of categories, in the order as passed into the constructor above\n",
    "# x_cont = torch.randn(1, 11)               # assume continuous values are already normalized individually\n",
    "\n",
    "# pred = model(x_categ, x_cont).to(device) # (1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_categ = torch.randint(0, 5, (1, 5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_categ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ix_categ = torch.Tensor([[2, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5]]).int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ix_categ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "summary(model,((1, 5),(1, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FTTransformer(\n",
      "  (categorical_embeds): Embedding(6, 16)\n",
      "  (numerical_embedder): NumericalEmbedder()\n",
      "  (transformer): Transformer(\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): Attention(\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          (to_qkv): Linear(in_features=16, out_features=1152, bias=False)\n",
      "          (to_out): Linear(in_features=384, out_features=16, bias=False)\n",
      "          (dropout): Dropout(p=0.3, inplace=False)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=16, out_features=128, bias=True)\n",
      "          (2): GEGLU()\n",
      "          (3): Dropout(p=0.3, inplace=False)\n",
      "          (4): Linear(in_features=64, out_features=16, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (1): ModuleList(\n",
      "        (0): Attention(\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          (to_qkv): Linear(in_features=16, out_features=1152, bias=False)\n",
      "          (to_out): Linear(in_features=384, out_features=16, bias=False)\n",
      "          (dropout): Dropout(p=0.3, inplace=False)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=16, out_features=128, bias=True)\n",
      "          (2): GEGLU()\n",
      "          (3): Dropout(p=0.3, inplace=False)\n",
      "          (4): Linear(in_features=64, out_features=16, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (2): ModuleList(\n",
      "        (0): Attention(\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          (to_qkv): Linear(in_features=16, out_features=1152, bias=False)\n",
      "          (to_out): Linear(in_features=384, out_features=16, bias=False)\n",
      "          (dropout): Dropout(p=0.3, inplace=False)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=16, out_features=128, bias=True)\n",
      "          (2): GEGLU()\n",
      "          (3): Dropout(p=0.3, inplace=False)\n",
      "          (4): Linear(in_features=64, out_features=16, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (to_logits): Sequential(\n",
      "    (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "def r2_torch_score(y_pred, y_true):\n",
    "    mean_y_true = torch.mean(y_true)\n",
    "    SS_res = torch.sum((y_true - y_pred) ** 2)\n",
    "    SS_tot = torch.sum((y_true - mean_y_true) ** 2)\n",
    "    r2 = 1 - (SS_res / SS_tot)\n",
    "    return r2\n",
    "#     return tf.clip_by_value(r2, 0, 1)\n",
    "\n",
    "def r2_torch_loss(y_pred, y_true):\n",
    "    mean_y_true = torch.mean(y_true)\n",
    "    SS_res = torch.sum((y_true - y_pred) ** 2)\n",
    "    SS_tot = torch.sum((y_true - mean_y_true) ** 2)\n",
    "    r2 = 1 - (SS_res / SS_tot)\n",
    "    return 1-r2\n",
    "#     return tf.clip_by_value((1-r2), 0, 1)\n",
    "\n",
    "\n",
    "def mape_loss(y_true, y_pred):\n",
    "    \n",
    "    if len(y_pred.shape)>=3:\n",
    "        y_pred = tf.squeeze(y_pred, axis=-1)\n",
    "        y_pred = tf.squeeze(y_pred, axis=-1)\n",
    "    if len(y_pred.shape)>=2:\n",
    "        y_pred = tf.squeeze(y_pred, axis=-1)\n",
    "        \n",
    "        \n",
    "    abs_diff = tf.abs(y_true-y_pred)/y_true\n",
    "    return tf.reduce_mean(abs_diff)*100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "optimizer = torch.optim.Adamax(model.parameters(), lr=0.01)\n",
    "\n",
    "input_data = torch.Tensor(X_train)\n",
    "labels = torch.Tensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "# Divide dataset into training and validation splits\n",
    "num_samples = len(input_data)\n",
    "indices = list(range(num_samples))\n",
    "random.shuffle(indices)\n",
    "split = int(num_samples * 0.975)  # 80% for training and 20% for validation  0.975\n",
    "train_idx, val_idx = indices[:split], indices[split:]\n",
    "train_inputs = input_data[train_idx]\n",
    "train_labels = labels[train_idx]\n",
    "val_inputs = input_data[val_idx]\n",
    "val_labels = labels[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDatasets for training and validation\n",
    "train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "val_dataset = TensorDataset(val_inputs, val_labels)\n",
    "\n",
    "# Define DataLoaders\n",
    "batch_size = 1  # choose a batch size\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Train Loss: 0.0177, Train R2 Loss: 0.4913, Val Loss: 3.1018, R2 Val : 0.5087  Time for epoch 1: 2.26 seconds\n",
      "Epoch [2/500], Train Loss: 0.1999, Train R2 Loss: 0.2303, Val Loss: 2.7277, R2 Val : 0.7697  Time for epoch 2: 1.69 seconds\n",
      "Epoch [3/500], Train Loss: 0.4216, Train R2 Loss: 0.4160, Val Loss: 3.6238, R2 Val : 0.5840  Time for epoch 3: 1.69 seconds\n",
      "Epoch [4/500], Train Loss: 0.2689, Train R2 Loss: 0.4090, Val Loss: 2.4630, R2 Val : 0.5910  Time for epoch 4: 1.68 seconds\n",
      "Epoch [5/500], Train Loss: 0.1728, Train R2 Loss: 0.2369, Val Loss: 2.2107, R2 Val : 0.7631  Time for epoch 5: 1.69 seconds\n",
      "Epoch [6/500], Train Loss: 0.1458, Train R2 Loss: 0.1737, Val Loss: 2.3944, R2 Val : 0.8263  Time for epoch 6: 1.67 seconds\n",
      "Epoch [7/500], Train Loss: 0.0090, Train R2 Loss: 0.2523, Val Loss: 2.0683, R2 Val : 0.7477  Time for epoch 7: 1.67 seconds\n",
      "Epoch [8/500], Train Loss: 2.7285, Train R2 Loss: 0.6455, Val Loss: 1.6624, R2 Val : 0.3545  Time for epoch 8: 1.67 seconds\n",
      "Epoch [9/500], Train Loss: 0.3903, Train R2 Loss: 0.9024, Val Loss: 4.1203, R2 Val : 0.0976  Time for epoch 9: 1.67 seconds\n",
      "Epoch [10/500], Train Loss: 0.1216, Train R2 Loss: 0.2249, Val Loss: 1.9687, R2 Val : 0.7751  Time for epoch 10: 1.68 seconds\n",
      "Epoch [11/500], Train Loss: 0.1643, Train R2 Loss: 0.2907, Val Loss: 1.9988, R2 Val : 0.7093  Time for epoch 11: 1.66 seconds\n",
      "Epoch [12/500], Train Loss: 0.3354, Train R2 Loss: 0.1988, Val Loss: 2.3109, R2 Val : 0.8012  Time for epoch 12: 1.66 seconds\n",
      "Epoch [13/500], Train Loss: 0.2797, Train R2 Loss: 0.3065, Val Loss: 4.3468, R2 Val : 0.6935  Time for epoch 13: 1.68 seconds\n",
      "Epoch [14/500], Train Loss: 0.2948, Train R2 Loss: 0.4203, Val Loss: 2.2334, R2 Val : 0.5797  Time for epoch 14: 1.66 seconds\n",
      "Epoch [15/500], Train Loss: 0.0248, Train R2 Loss: 0.4284, Val Loss: 2.8355, R2 Val : 0.5716  Time for epoch 15: 1.66 seconds\n",
      "Epoch [16/500], Train Loss: 0.2132, Train R2 Loss: 0.4283, Val Loss: 1.9973, R2 Val : 0.5717  Time for epoch 16: 1.68 seconds\n",
      "Epoch [17/500], Train Loss: 1.0473, Train R2 Loss: 1.0760, Val Loss: 1.5808, R2 Val : -0.0760  Time for epoch 17: 1.66 seconds\n",
      "Epoch [18/500], Train Loss: 0.0424, Train R2 Loss: 0.1506, Val Loss: 1.4606, R2 Val : 0.8494  Time for epoch 18: 1.67 seconds\n",
      "Epoch [19/500], Train Loss: 0.2633, Train R2 Loss: 0.3669, Val Loss: 1.5277, R2 Val : 0.6331  Time for epoch 19: 1.68 seconds\n",
      "Epoch [20/500], Train Loss: 0.2150, Train R2 Loss: 0.3696, Val Loss: 1.7664, R2 Val : 0.6304  Time for epoch 20: 1.67 seconds\n",
      "Epoch [21/500], Train Loss: 1.5909, Train R2 Loss: 0.2304, Val Loss: 1.9429, R2 Val : 0.7696  Time for epoch 21: 1.69 seconds\n",
      "Epoch [22/500], Train Loss: 0.7376, Train R2 Loss: 1.0696, Val Loss: 3.2160, R2 Val : -0.0696  Time for epoch 22: 1.67 seconds\n",
      "Epoch [23/500], Train Loss: 0.7006, Train R2 Loss: 0.7954, Val Loss: 1.8774, R2 Val : 0.2046  Time for epoch 23: 1.67 seconds\n",
      "Epoch [24/500], Train Loss: 0.2045, Train R2 Loss: 0.1266, Val Loss: 2.0278, R2 Val : 0.8734  Time for epoch 24: 1.67 seconds\n",
      "Epoch [25/500], Train Loss: 0.2243, Train R2 Loss: 0.4714, Val Loss: 1.9793, R2 Val : 0.5286  Time for epoch 25: 1.70 seconds\n",
      "Epoch [26/500], Train Loss: 0.1383, Train R2 Loss: 0.1830, Val Loss: 2.4214, R2 Val : 0.8170  Time for epoch 26: 1.68 seconds\n",
      "Epoch [27/500], Train Loss: 0.7452, Train R2 Loss: 0.9603, Val Loss: 3.9030, R2 Val : 0.0397  Time for epoch 27: 1.67 seconds\n",
      "Epoch [28/500], Train Loss: 0.1797, Train R2 Loss: 0.2466, Val Loss: 2.0660, R2 Val : 0.7534  Time for epoch 28: 1.67 seconds\n",
      "Epoch [29/500], Train Loss: 0.8017, Train R2 Loss: 0.6068, Val Loss: 3.4461, R2 Val : 0.3932  Time for epoch 29: 1.68 seconds\n",
      "Epoch [30/500], Train Loss: 0.3257, Train R2 Loss: 0.3371, Val Loss: 1.8953, R2 Val : 0.6629  Time for epoch 30: 1.68 seconds\n",
      "Epoch [31/500], Train Loss: 0.3349, Train R2 Loss: 0.3053, Val Loss: 3.2584, R2 Val : 0.6947  Time for epoch 31: 1.67 seconds\n",
      "Epoch [32/500], Train Loss: 0.6498, Train R2 Loss: 0.5924, Val Loss: 2.6285, R2 Val : 0.4076  Time for epoch 32: 1.70 seconds\n",
      "Epoch [33/500], Train Loss: 0.4520, Train R2 Loss: 0.8319, Val Loss: 2.2408, R2 Val : 0.1681  Time for epoch 33: 1.69 seconds\n",
      "Epoch [34/500], Train Loss: 0.5061, Train R2 Loss: 0.8081, Val Loss: 2.3868, R2 Val : 0.1919  Time for epoch 34: 1.69 seconds\n",
      "Epoch [35/500], Train Loss: 0.5418, Train R2 Loss: 0.7366, Val Loss: 2.0160, R2 Val : 0.2634  Time for epoch 35: 1.68 seconds\n",
      "Epoch [36/500], Train Loss: 0.7487, Train R2 Loss: 1.6057, Val Loss: 3.7295, R2 Val : -0.6057  Time for epoch 36: 1.67 seconds\n",
      "Epoch [37/500], Train Loss: 0.0095, Train R2 Loss: 0.0817, Val Loss: 1.9268, R2 Val : 0.9183  Time for epoch 37: 1.69 seconds\n",
      "Epoch [38/500], Train Loss: 0.0382, Train R2 Loss: 0.0870, Val Loss: 1.8867, R2 Val : 0.9130  Time for epoch 38: 1.78 seconds\n",
      "Epoch [39/500], Train Loss: 0.0453, Train R2 Loss: 0.2730, Val Loss: 1.8925, R2 Val : 0.7270  Time for epoch 39: 1.68 seconds\n",
      "Epoch [40/500], Train Loss: 0.0583, Train R2 Loss: 0.4368, Val Loss: 2.3638, R2 Val : 0.5632  Time for epoch 40: 1.83 seconds\n",
      "Epoch [41/500], Train Loss: 0.4188, Train R2 Loss: 0.6544, Val Loss: 2.9508, R2 Val : 0.3456  Time for epoch 41: 1.74 seconds\n",
      "Epoch [42/500], Train Loss: 0.0541, Train R2 Loss: 0.4927, Val Loss: 2.8288, R2 Val : 0.5073  Time for epoch 42: 1.75 seconds\n",
      "Epoch [43/500], Train Loss: 0.3780, Train R2 Loss: 0.3425, Val Loss: 4.0880, R2 Val : 0.6575  Time for epoch 43: 1.73 seconds\n",
      "Epoch [44/500], Train Loss: 0.8476, Train R2 Loss: 1.9356, Val Loss: 3.1410, R2 Val : -0.9356  Time for epoch 44: 1.79 seconds\n",
      "Epoch [45/500], Train Loss: 0.0717, Train R2 Loss: 0.1543, Val Loss: 1.5721, R2 Val : 0.8457  Time for epoch 45: 1.70 seconds\n",
      "Epoch [46/500], Train Loss: 0.3045, Train R2 Loss: 0.2421, Val Loss: 3.8559, R2 Val : 0.7579  Time for epoch 46: 1.71 seconds\n",
      "Epoch [47/500], Train Loss: 0.1173, Train R2 Loss: 0.1171, Val Loss: 1.8364, R2 Val : 0.8829  Time for epoch 47: 1.69 seconds\n",
      "Epoch [48/500], Train Loss: 0.8139, Train R2 Loss: 1.3694, Val Loss: 2.4147, R2 Val : -0.3694  Time for epoch 48: 1.67 seconds\n",
      "Epoch [49/500], Train Loss: 0.0229, Train R2 Loss: 0.4356, Val Loss: 2.3534, R2 Val : 0.5644  Time for epoch 49: 1.69 seconds\n",
      "Epoch [50/500], Train Loss: 0.0179, Train R2 Loss: 0.4967, Val Loss: 2.4297, R2 Val : 0.5033  Time for epoch 50: 1.68 seconds\n",
      "Epoch [51/500], Train Loss: 0.7947, Train R2 Loss: 0.6637, Val Loss: 1.9176, R2 Val : 0.3363  Time for epoch 51: 1.68 seconds\n",
      "Epoch [52/500], Train Loss: 0.3965, Train R2 Loss: 0.5361, Val Loss: 2.2521, R2 Val : 0.4639  Time for epoch 52: 1.68 seconds\n",
      "Epoch [53/500], Train Loss: 0.0278, Train R2 Loss: 0.2804, Val Loss: 2.6778, R2 Val : 0.7196  Time for epoch 53: 1.69 seconds\n",
      "Epoch [54/500], Train Loss: 0.0901, Train R2 Loss: 0.2893, Val Loss: 2.2304, R2 Val : 0.7107  Time for epoch 54: 1.70 seconds\n",
      "Epoch [55/500], Train Loss: 0.1221, Train R2 Loss: 0.2790, Val Loss: 2.1382, R2 Val : 0.7210  Time for epoch 55: 1.69 seconds\n",
      "Epoch [56/500], Train Loss: 0.3826, Train R2 Loss: 1.1123, Val Loss: 4.0803, R2 Val : -0.1123  Time for epoch 56: 1.67 seconds\n",
      "Epoch [57/500], Train Loss: 0.5727, Train R2 Loss: 0.7937, Val Loss: 2.3163, R2 Val : 0.2063  Time for epoch 57: 1.70 seconds\n",
      "Epoch [58/500], Train Loss: 0.0703, Train R2 Loss: 0.2006, Val Loss: 2.3632, R2 Val : 0.7994  Time for epoch 58: 1.72 seconds\n",
      "Epoch [59/500], Train Loss: 0.0643, Train R2 Loss: 0.2532, Val Loss: 2.1928, R2 Val : 0.7468  Time for epoch 59: 1.69 seconds\n",
      "Epoch [60/500], Train Loss: 0.1263, Train R2 Loss: 0.2334, Val Loss: 2.0447, R2 Val : 0.7666  Time for epoch 60: 1.73 seconds\n",
      "Epoch [61/500], Train Loss: 0.0313, Train R2 Loss: 0.4174, Val Loss: 1.9708, R2 Val : 0.5826  Time for epoch 61: 1.69 seconds\n",
      "Epoch [62/500], Train Loss: 0.0102, Train R2 Loss: 0.2085, Val Loss: 1.9436, R2 Val : 0.7915  Time for epoch 62: 1.69 seconds\n",
      "Epoch [63/500], Train Loss: 0.1534, Train R2 Loss: 0.4827, Val Loss: 3.0494, R2 Val : 0.5173  Time for epoch 63: 1.73 seconds\n",
      "Epoch [64/500], Train Loss: 0.1076, Train R2 Loss: 0.2492, Val Loss: 2.0103, R2 Val : 0.7508  Time for epoch 64: 1.78 seconds\n",
      "Epoch [65/500], Train Loss: 0.0599, Train R2 Loss: 0.6021, Val Loss: 3.5957, R2 Val : 0.3979  Time for epoch 65: 1.80 seconds\n",
      "Epoch [66/500], Train Loss: 0.1698, Train R2 Loss: 0.1903, Val Loss: 1.8600, R2 Val : 0.8097  Time for epoch 66: 1.75 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [67/500], Train Loss: 0.1955, Train R2 Loss: 0.1785, Val Loss: 1.8656, R2 Val : 0.8215  Time for epoch 67: 1.68 seconds\n",
      "Epoch [68/500], Train Loss: 0.3299, Train R2 Loss: 0.1687, Val Loss: 2.2242, R2 Val : 0.8313  Time for epoch 68: 1.67 seconds\n",
      "Epoch [69/500], Train Loss: 0.0258, Train R2 Loss: 0.4188, Val Loss: 1.9613, R2 Val : 0.5812  Time for epoch 69: 1.69 seconds\n",
      "Epoch [70/500], Train Loss: 0.1033, Train R2 Loss: 0.0872, Val Loss: 1.8056, R2 Val : 0.9128  Time for epoch 70: 1.68 seconds\n",
      "Epoch [71/500], Train Loss: 0.0106, Train R2 Loss: 0.3652, Val Loss: 1.8596, R2 Val : 0.6348  Time for epoch 71: 1.67 seconds\n",
      "Epoch [72/500], Train Loss: 0.1703, Train R2 Loss: 0.2605, Val Loss: 2.8478, R2 Val : 0.7395  Time for epoch 72: 1.69 seconds\n",
      "Epoch [73/500], Train Loss: 0.1548, Train R2 Loss: 0.2097, Val Loss: 3.7761, R2 Val : 0.7903  Time for epoch 73: 1.69 seconds\n",
      "Epoch [74/500], Train Loss: 0.3764, Train R2 Loss: 0.2157, Val Loss: 2.5882, R2 Val : 0.7843  Time for epoch 74: 1.67 seconds\n",
      "Epoch [75/500], Train Loss: 0.5043, Train R2 Loss: 0.4801, Val Loss: 2.3063, R2 Val : 0.5199  Time for epoch 75: 1.68 seconds\n",
      "Epoch [76/500], Train Loss: 0.4105, Train R2 Loss: 1.9271, Val Loss: 4.9654, R2 Val : -0.9271  Time for epoch 76: 1.69 seconds\n",
      "Epoch [77/500], Train Loss: 0.1047, Train R2 Loss: 0.0279, Val Loss: 2.0428, R2 Val : 0.9721  Time for epoch 77: 1.67 seconds\n",
      "Epoch [78/500], Train Loss: 0.0931, Train R2 Loss: 0.3821, Val Loss: 2.2166, R2 Val : 0.6179  Time for epoch 78: 1.67 seconds\n",
      "Epoch [79/500], Train Loss: 0.2384, Train R2 Loss: 0.6696, Val Loss: 3.6063, R2 Val : 0.3304  Time for epoch 79: 1.69 seconds\n",
      "Epoch [80/500], Train Loss: 0.1453, Train R2 Loss: 0.2301, Val Loss: 2.6733, R2 Val : 0.7699  Time for epoch 80: 1.67 seconds\n",
      "Epoch [81/500], Train Loss: 0.3196, Train R2 Loss: 0.3843, Val Loss: 1.9675, R2 Val : 0.6157  Time for epoch 81: 1.71 seconds\n",
      "Epoch [82/500], Train Loss: 0.0787, Train R2 Loss: 0.1804, Val Loss: 2.2803, R2 Val : 0.8196  Time for epoch 82: 1.70 seconds\n",
      "Epoch [83/500], Train Loss: 0.1439, Train R2 Loss: 0.1925, Val Loss: 2.2079, R2 Val : 0.8075  Time for epoch 83: 1.69 seconds\n",
      "Epoch [84/500], Train Loss: 0.2565, Train R2 Loss: 0.2966, Val Loss: 2.0510, R2 Val : 0.7034  Time for epoch 84: 1.68 seconds\n",
      "Epoch [85/500], Train Loss: 0.1163, Train R2 Loss: 0.2759, Val Loss: 1.8701, R2 Val : 0.7241  Time for epoch 85: 1.70 seconds\n",
      "Epoch [86/500], Train Loss: 0.0741, Train R2 Loss: 0.3208, Val Loss: 1.9687, R2 Val : 0.6792  Time for epoch 86: 1.67 seconds\n",
      "Epoch [87/500], Train Loss: 0.0110, Train R2 Loss: 0.4257, Val Loss: 1.8967, R2 Val : 0.5743  Time for epoch 87: 1.68 seconds\n",
      "Epoch [88/500], Train Loss: 0.0317, Train R2 Loss: 0.1541, Val Loss: 1.9483, R2 Val : 0.8459  Time for epoch 88: 1.67 seconds\n",
      "Epoch [89/500], Train Loss: 0.0241, Train R2 Loss: 0.3414, Val Loss: 2.0301, R2 Val : 0.6586  Time for epoch 89: 1.68 seconds\n",
      "Epoch [90/500], Train Loss: 0.2643, Train R2 Loss: 0.7694, Val Loss: 2.4070, R2 Val : 0.2306  Time for epoch 90: 1.67 seconds\n",
      "Epoch [91/500], Train Loss: 0.1496, Train R2 Loss: 0.4197, Val Loss: 1.9483, R2 Val : 0.5803  Time for epoch 91: 1.68 seconds\n",
      "Epoch [92/500], Train Loss: 0.0590, Train R2 Loss: 0.4303, Val Loss: 3.9313, R2 Val : 0.5697  Time for epoch 92: 1.68 seconds\n",
      "Epoch [93/500], Train Loss: 0.0582, Train R2 Loss: 0.1777, Val Loss: 2.1216, R2 Val : 0.8223  Time for epoch 93: 1.68 seconds\n",
      "Epoch [94/500], Train Loss: 0.0745, Train R2 Loss: 0.7929, Val Loss: 2.6859, R2 Val : 0.2071  Time for epoch 94: 1.68 seconds\n",
      "Epoch [95/500], Train Loss: 0.0568, Train R2 Loss: 0.3011, Val Loss: 2.0525, R2 Val : 0.6989  Time for epoch 95: 1.68 seconds\n",
      "Epoch [96/500], Train Loss: 0.4128, Train R2 Loss: 0.8494, Val Loss: 2.3005, R2 Val : 0.1506  Time for epoch 96: 1.70 seconds\n",
      "Epoch [97/500], Train Loss: 0.0673, Train R2 Loss: 0.3575, Val Loss: 2.1254, R2 Val : 0.6425  Time for epoch 97: 1.67 seconds\n",
      "Epoch [98/500], Train Loss: 0.0060, Train R2 Loss: 0.1888, Val Loss: 2.3083, R2 Val : 0.8112  Time for epoch 98: 1.68 seconds\n",
      "Epoch [99/500], Train Loss: 0.0939, Train R2 Loss: 0.4885, Val Loss: 2.4969, R2 Val : 0.5115  Time for epoch 99: 1.67 seconds\n",
      "Epoch [100/500], Train Loss: 0.1161, Train R2 Loss: 0.4829, Val Loss: 2.7645, R2 Val : 0.5171  Time for epoch 100: 1.67 seconds\n",
      "Epoch [101/500], Train Loss: 0.3221, Train R2 Loss: 0.5474, Val Loss: 2.3033, R2 Val : 0.4526  Time for epoch 101: 1.68 seconds\n",
      "Epoch [102/500], Train Loss: 0.0145, Train R2 Loss: 0.1926, Val Loss: 2.0041, R2 Val : 0.8074  Time for epoch 102: 1.67 seconds\n",
      "Epoch [103/500], Train Loss: 0.0580, Train R2 Loss: 0.1746, Val Loss: 2.0688, R2 Val : 0.8254  Time for epoch 103: 1.68 seconds\n",
      "Epoch [104/500], Train Loss: 0.6593, Train R2 Loss: 0.8362, Val Loss: 2.0320, R2 Val : 0.1638  Time for epoch 104: 1.68 seconds\n",
      "Epoch [105/500], Train Loss: 0.0376, Train R2 Loss: 0.6377, Val Loss: 2.7223, R2 Val : 0.3623  Time for epoch 105: 1.67 seconds\n",
      "Epoch [106/500], Train Loss: 0.3843, Train R2 Loss: 0.3396, Val Loss: 2.4677, R2 Val : 0.6604  Time for epoch 106: 1.68 seconds\n",
      "Epoch [107/500], Train Loss: 0.0427, Train R2 Loss: 0.4529, Val Loss: 2.2393, R2 Val : 0.5471  Time for epoch 107: 1.68 seconds\n",
      "Epoch [108/500], Train Loss: 0.1307, Train R2 Loss: 0.2974, Val Loss: 2.4981, R2 Val : 0.7026  Time for epoch 108: 1.69 seconds\n",
      "Epoch [109/500], Train Loss: 0.1224, Train R2 Loss: 0.6679, Val Loss: 2.7598, R2 Val : 0.3321  Time for epoch 109: 1.68 seconds\n",
      "Epoch [110/500], Train Loss: 0.1558, Train R2 Loss: 0.3603, Val Loss: 2.2767, R2 Val : 0.6397  Time for epoch 110: 1.68 seconds\n",
      "Epoch [111/500], Train Loss: 0.3996, Train R2 Loss: 0.2766, Val Loss: 2.1273, R2 Val : 0.7234  Time for epoch 111: 1.67 seconds\n",
      "Epoch [112/500], Train Loss: 0.1479, Train R2 Loss: 0.2171, Val Loss: 2.1348, R2 Val : 0.7829  Time for epoch 112: 1.68 seconds\n",
      "Epoch [113/500], Train Loss: 0.0843, Train R2 Loss: 0.1885, Val Loss: 2.2838, R2 Val : 0.8115  Time for epoch 113: 1.68 seconds\n",
      "Epoch [114/500], Train Loss: 0.4278, Train R2 Loss: 0.4190, Val Loss: 2.1193, R2 Val : 0.5810  Time for epoch 114: 1.68 seconds\n",
      "Epoch [115/500], Train Loss: 0.0923, Train R2 Loss: 0.3187, Val Loss: 2.0015, R2 Val : 0.6813  Time for epoch 115: 1.68 seconds\n",
      "Epoch [116/500], Train Loss: 0.0073, Train R2 Loss: 0.3694, Val Loss: 2.3846, R2 Val : 0.6306  Time for epoch 116: 1.67 seconds\n",
      "Epoch [117/500], Train Loss: 0.0103, Train R2 Loss: 0.2800, Val Loss: 1.7115, R2 Val : 0.7200  Time for epoch 117: 1.69 seconds\n",
      "Epoch [118/500], Train Loss: 0.0014, Train R2 Loss: 0.1793, Val Loss: 2.1101, R2 Val : 0.8207  Time for epoch 118: 1.70 seconds\n",
      "Epoch [119/500], Train Loss: 0.2735, Train R2 Loss: 0.6121, Val Loss: 2.1855, R2 Val : 0.3879  Time for epoch 119: 1.68 seconds\n",
      "Epoch [120/500], Train Loss: 0.6032, Train R2 Loss: 0.5465, Val Loss: 2.1922, R2 Val : 0.4535  Time for epoch 120: 1.70 seconds\n",
      "Epoch [121/500], Train Loss: 0.1031, Train R2 Loss: 0.4388, Val Loss: 2.5915, R2 Val : 0.5612  Time for epoch 121: 1.69 seconds\n",
      "Epoch [122/500], Train Loss: 0.1339, Train R2 Loss: 0.2212, Val Loss: 1.9794, R2 Val : 0.7788  Time for epoch 122: 1.70 seconds\n",
      "Epoch [123/500], Train Loss: 0.0434, Train R2 Loss: 0.2580, Val Loss: 2.4303, R2 Val : 0.7420  Time for epoch 123: 1.69 seconds\n",
      "Epoch [124/500], Train Loss: 0.4661, Train R2 Loss: 0.7214, Val Loss: 2.1209, R2 Val : 0.2786  Time for epoch 124: 1.68 seconds\n",
      "Epoch [125/500], Train Loss: 0.0860, Train R2 Loss: 0.1979, Val Loss: 2.0305, R2 Val : 0.8021  Time for epoch 125: 1.67 seconds\n",
      "Epoch [126/500], Train Loss: 0.0418, Train R2 Loss: 0.1999, Val Loss: 2.2310, R2 Val : 0.8001  Time for epoch 126: 1.68 seconds\n",
      "Epoch [127/500], Train Loss: 0.5318, Train R2 Loss: 0.9655, Val Loss: 2.2838, R2 Val : 0.0345  Time for epoch 127: 1.69 seconds\n",
      "Epoch [128/500], Train Loss: 0.2096, Train R2 Loss: 0.3832, Val Loss: 1.9920, R2 Val : 0.6168  Time for epoch 128: 1.68 seconds\n",
      "Epoch [129/500], Train Loss: 0.0004, Train R2 Loss: 0.1680, Val Loss: 2.1073, R2 Val : 0.8320  Time for epoch 129: 1.68 seconds\n",
      "Epoch [130/500], Train Loss: 0.0062, Train R2 Loss: 0.4798, Val Loss: 2.4458, R2 Val : 0.5202  Time for epoch 130: 1.67 seconds\n",
      "Epoch [131/500], Train Loss: 0.0248, Train R2 Loss: 0.2979, Val Loss: 1.9167, R2 Val : 0.7021  Time for epoch 131: 1.68 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [132/500], Train Loss: 0.2059, Train R2 Loss: 0.6432, Val Loss: 2.3578, R2 Val : 0.3568  Time for epoch 132: 1.67 seconds\n",
      "Epoch [133/500], Train Loss: 0.1650, Train R2 Loss: 0.5509, Val Loss: 2.1256, R2 Val : 0.4491  Time for epoch 133: 1.68 seconds\n",
      "Epoch [134/500], Train Loss: 0.1714, Train R2 Loss: 0.2650, Val Loss: 2.3392, R2 Val : 0.7350  Time for epoch 134: 1.67 seconds\n",
      "Epoch [135/500], Train Loss: 0.0107, Train R2 Loss: 0.4554, Val Loss: 2.0749, R2 Val : 0.5446  Time for epoch 135: 1.67 seconds\n",
      "Epoch [136/500], Train Loss: 0.0534, Train R2 Loss: 0.1291, Val Loss: 2.2602, R2 Val : 0.8709  Time for epoch 136: 1.68 seconds\n",
      "Epoch [137/500], Train Loss: 0.1006, Train R2 Loss: 0.4942, Val Loss: 1.9952, R2 Val : 0.5058  Time for epoch 137: 1.67 seconds\n",
      "Epoch [138/500], Train Loss: 0.2347, Train R2 Loss: 0.5111, Val Loss: 2.3487, R2 Val : 0.4889  Time for epoch 138: 1.67 seconds\n",
      "Epoch [139/500], Train Loss: 0.3309, Train R2 Loss: 0.4685, Val Loss: 2.2817, R2 Val : 0.5315  Time for epoch 139: 1.68 seconds\n",
      "Epoch [140/500], Train Loss: 0.1031, Train R2 Loss: 0.2664, Val Loss: 2.2175, R2 Val : 0.7336  Time for epoch 140: 1.67 seconds\n",
      "Epoch [141/500], Train Loss: 0.0244, Train R2 Loss: 0.4015, Val Loss: 1.8790, R2 Val : 0.5985  Time for epoch 141: 1.68 seconds\n",
      "Epoch [142/500], Train Loss: 0.0947, Train R2 Loss: 0.2216, Val Loss: 2.0212, R2 Val : 0.7784  Time for epoch 142: 1.68 seconds\n",
      "Epoch [143/500], Train Loss: 0.0809, Train R2 Loss: 0.3003, Val Loss: 1.7827, R2 Val : 0.6997  Time for epoch 143: 1.67 seconds\n",
      "Epoch [144/500], Train Loss: 0.1208, Train R2 Loss: 0.5339, Val Loss: 2.4092, R2 Val : 0.4661  Time for epoch 144: 1.67 seconds\n",
      "Epoch [145/500], Train Loss: 0.4767, Train R2 Loss: 0.3170, Val Loss: 1.6848, R2 Val : 0.6830  Time for epoch 145: 1.67 seconds\n",
      "Epoch [146/500], Train Loss: 0.0954, Train R2 Loss: 0.1096, Val Loss: 1.7108, R2 Val : 0.8904  Time for epoch 146: 1.67 seconds\n",
      "Epoch [147/500], Train Loss: 0.0426, Train R2 Loss: 0.1333, Val Loss: 2.0833, R2 Val : 0.8667  Time for epoch 147: 1.67 seconds\n",
      "Epoch [148/500], Train Loss: 0.1791, Train R2 Loss: 0.1400, Val Loss: 1.7283, R2 Val : 0.8600  Time for epoch 148: 1.68 seconds\n",
      "Epoch [149/500], Train Loss: 0.1845, Train R2 Loss: 0.3185, Val Loss: 1.7021, R2 Val : 0.6815  Time for epoch 149: 1.67 seconds\n",
      "Epoch [150/500], Train Loss: 0.3198, Train R2 Loss: 0.4634, Val Loss: 1.8904, R2 Val : 0.5366  Time for epoch 150: 1.67 seconds\n",
      "Epoch [151/500], Train Loss: 0.0616, Train R2 Loss: 0.2062, Val Loss: 1.8731, R2 Val : 0.7938  Time for epoch 151: 1.67 seconds\n",
      "Epoch [152/500], Train Loss: 0.0084, Train R2 Loss: 0.1897, Val Loss: 1.9166, R2 Val : 0.8103  Time for epoch 152: 1.68 seconds\n",
      "Epoch [153/500], Train Loss: 0.0837, Train R2 Loss: 0.3145, Val Loss: 1.7234, R2 Val : 0.6855  Time for epoch 153: 1.67 seconds\n",
      "Epoch [154/500], Train Loss: 0.0638, Train R2 Loss: 0.0224, Val Loss: 1.6766, R2 Val : 0.9776  Time for epoch 154: 1.67 seconds\n",
      "Epoch [155/500], Train Loss: 0.0605, Train R2 Loss: 0.2735, Val Loss: 1.9736, R2 Val : 0.7265  Time for epoch 155: 1.68 seconds\n",
      "Epoch [156/500], Train Loss: 0.1801, Train R2 Loss: 0.2580, Val Loss: 3.0380, R2 Val : 0.7420  Time for epoch 156: 1.68 seconds\n",
      "Epoch [157/500], Train Loss: 0.4216, Train R2 Loss: 0.4707, Val Loss: 1.4796, R2 Val : 0.5293  Time for epoch 157: 1.67 seconds\n",
      "Epoch [158/500], Train Loss: 0.8787, Train R2 Loss: 0.9880, Val Loss: 1.5715, R2 Val : 0.0120  Time for epoch 158: 1.68 seconds\n",
      "Epoch [159/500], Train Loss: 0.0324, Train R2 Loss: 0.1883, Val Loss: 1.6557, R2 Val : 0.8117  Time for epoch 159: 1.67 seconds\n",
      "Epoch [160/500], Train Loss: 0.4791, Train R2 Loss: 0.6964, Val Loss: 1.6574, R2 Val : 0.3036  Time for epoch 160: 1.67 seconds\n",
      "Epoch [161/500], Train Loss: 0.5533, Train R2 Loss: 1.0368, Val Loss: 2.5776, R2 Val : -0.0368  Time for epoch 161: 1.68 seconds\n",
      "Epoch [162/500], Train Loss: 0.0868, Train R2 Loss: 0.1706, Val Loss: 1.5248, R2 Val : 0.8294  Time for epoch 162: 1.67 seconds\n",
      "Epoch [163/500], Train Loss: 0.0372, Train R2 Loss: 0.1287, Val Loss: 1.4661, R2 Val : 0.8713  Time for epoch 163: 1.67 seconds\n",
      "Epoch [164/500], Train Loss: 0.0782, Train R2 Loss: 0.1636, Val Loss: 1.7353, R2 Val : 0.8364  Time for epoch 164: 1.68 seconds\n",
      "Epoch [165/500], Train Loss: 0.0012, Train R2 Loss: 0.2016, Val Loss: 1.8841, R2 Val : 0.7984  Time for epoch 165: 1.67 seconds\n",
      "Epoch [166/500], Train Loss: 0.1501, Train R2 Loss: 0.2226, Val Loss: 1.7398, R2 Val : 0.7774  Time for epoch 166: 1.67 seconds\n",
      "Epoch [167/500], Train Loss: 0.4496, Train R2 Loss: 0.5817, Val Loss: 2.4375, R2 Val : 0.4183  Time for epoch 167: 1.68 seconds\n",
      "Epoch [168/500], Train Loss: 0.1096, Train R2 Loss: 0.1420, Val Loss: 2.1670, R2 Val : 0.8580  Time for epoch 168: 1.67 seconds\n",
      "Epoch [169/500], Train Loss: 0.1229, Train R2 Loss: 0.2925, Val Loss: 1.7887, R2 Val : 0.7075  Time for epoch 169: 1.67 seconds\n",
      "Epoch [170/500], Train Loss: 0.1568, Train R2 Loss: 0.2941, Val Loss: 1.6508, R2 Val : 0.7059  Time for epoch 170: 1.67 seconds\n",
      "Epoch [171/500], Train Loss: 0.0709, Train R2 Loss: 0.3609, Val Loss: 2.0096, R2 Val : 0.6391  Time for epoch 171: 1.68 seconds\n",
      "Epoch [172/500], Train Loss: 0.1196, Train R2 Loss: 0.1251, Val Loss: 1.6730, R2 Val : 0.8749  Time for epoch 172: 1.67 seconds\n",
      "Epoch [173/500], Train Loss: 0.1420, Train R2 Loss: 0.2758, Val Loss: 1.5660, R2 Val : 0.7242  Time for epoch 173: 1.67 seconds\n",
      "Epoch [174/500], Train Loss: 0.0050, Train R2 Loss: 0.3569, Val Loss: 2.0597, R2 Val : 0.6431  Time for epoch 174: 1.68 seconds\n",
      "Epoch [175/500], Train Loss: 0.0583, Train R2 Loss: 0.5433, Val Loss: 1.9681, R2 Val : 0.4567  Time for epoch 175: 1.67 seconds\n",
      "Epoch [176/500], Train Loss: 0.0063, Train R2 Loss: 0.1445, Val Loss: 2.2623, R2 Val : 0.8555  Time for epoch 176: 1.67 seconds\n",
      "Epoch [177/500], Train Loss: 0.3142, Train R2 Loss: 0.5499, Val Loss: 1.8739, R2 Val : 0.4501  Time for epoch 177: 1.68 seconds\n",
      "Epoch [178/500], Train Loss: 0.1402, Train R2 Loss: 0.0912, Val Loss: 1.6409, R2 Val : 0.9088  Time for epoch 178: 1.69 seconds\n",
      "Epoch [179/500], Train Loss: 0.0057, Train R2 Loss: 0.1799, Val Loss: 1.6291, R2 Val : 0.8201  Time for epoch 179: 1.67 seconds\n",
      "Epoch [180/500], Train Loss: 0.0578, Train R2 Loss: 0.2258, Val Loss: 1.5971, R2 Val : 0.7742  Time for epoch 180: 1.68 seconds\n",
      "Epoch [181/500], Train Loss: 0.8180, Train R2 Loss: 0.9420, Val Loss: 1.9020, R2 Val : 0.0580  Time for epoch 181: 1.67 seconds\n",
      "Epoch [182/500], Train Loss: 0.0049, Train R2 Loss: 0.3153, Val Loss: 2.1224, R2 Val : 0.6847  Time for epoch 182: 1.67 seconds\n",
      "Epoch [183/500], Train Loss: 0.0421, Train R2 Loss: 0.2264, Val Loss: 1.5017, R2 Val : 0.7736  Time for epoch 183: 1.68 seconds\n",
      "Epoch [184/500], Train Loss: 0.3426, Train R2 Loss: 0.2779, Val Loss: 1.9747, R2 Val : 0.7221  Time for epoch 184: 1.67 seconds\n",
      "Epoch [185/500], Train Loss: 0.0039, Train R2 Loss: 0.3000, Val Loss: 2.0976, R2 Val : 0.7000  Time for epoch 185: 1.67 seconds\n",
      "Epoch [186/500], Train Loss: 0.0308, Train R2 Loss: 0.4531, Val Loss: 2.1626, R2 Val : 0.5469  Time for epoch 186: 1.68 seconds\n",
      "Epoch [187/500], Train Loss: 0.0084, Train R2 Loss: 0.5162, Val Loss: 2.4576, R2 Val : 0.4838  Time for epoch 187: 1.67 seconds\n",
      "Epoch [188/500], Train Loss: 0.0277, Train R2 Loss: 0.1245, Val Loss: 1.8130, R2 Val : 0.8755  Time for epoch 188: 1.67 seconds\n",
      "Epoch [189/500], Train Loss: 0.0750, Train R2 Loss: 0.2537, Val Loss: 1.4848, R2 Val : 0.7463  Time for epoch 189: 1.67 seconds\n",
      "Epoch [190/500], Train Loss: 0.0759, Train R2 Loss: 0.1306, Val Loss: 1.7324, R2 Val : 0.8694  Time for epoch 190: 1.67 seconds\n",
      "Epoch [191/500], Train Loss: 0.0910, Train R2 Loss: 0.3810, Val Loss: 2.7161, R2 Val : 0.6190  Time for epoch 191: 1.67 seconds\n",
      "Epoch [192/500], Train Loss: 0.0624, Train R2 Loss: 0.1325, Val Loss: 1.6505, R2 Val : 0.8675  Time for epoch 192: 1.68 seconds\n",
      "Epoch [193/500], Train Loss: 0.0342, Train R2 Loss: 0.2829, Val Loss: 1.3987, R2 Val : 0.7171  Time for epoch 193: 1.69 seconds\n",
      "Epoch [194/500], Train Loss: 0.0392, Train R2 Loss: 0.0732, Val Loss: 1.4861, R2 Val : 0.9268  Time for epoch 194: 1.69 seconds\n",
      "Epoch [195/500], Train Loss: 0.0951, Train R2 Loss: 0.1643, Val Loss: 2.4514, R2 Val : 0.8357  Time for epoch 195: 1.69 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [196/500], Train Loss: 0.0097, Train R2 Loss: 0.0970, Val Loss: 1.3241, R2 Val : 0.9030  Time for epoch 196: 1.74 seconds\n",
      "Epoch [197/500], Train Loss: 0.0425, Train R2 Loss: 0.2377, Val Loss: 1.7642, R2 Val : 0.7623  Time for epoch 197: 1.69 seconds\n",
      "Epoch [198/500], Train Loss: 0.0255, Train R2 Loss: 0.0827, Val Loss: 1.4434, R2 Val : 0.9173  Time for epoch 198: 1.68 seconds\n",
      "Epoch [199/500], Train Loss: 0.0434, Train R2 Loss: 0.1157, Val Loss: 1.5073, R2 Val : 0.8843  Time for epoch 199: 1.70 seconds\n",
      "Epoch [200/500], Train Loss: 0.3298, Train R2 Loss: 0.5589, Val Loss: 3.2219, R2 Val : 0.4411  Time for epoch 200: 1.68 seconds\n",
      "Epoch [201/500], Train Loss: 0.0832, Train R2 Loss: 0.0666, Val Loss: 1.4353, R2 Val : 0.9334  Time for epoch 201: 1.69 seconds\n",
      "Epoch [202/500], Train Loss: 0.2806, Train R2 Loss: 0.3387, Val Loss: 1.2694, R2 Val : 0.6613  Time for epoch 202: 1.68 seconds\n",
      "Epoch [203/500], Train Loss: 0.1140, Train R2 Loss: 0.4208, Val Loss: 1.8747, R2 Val : 0.5792  Time for epoch 203: 1.69 seconds\n",
      "Epoch [204/500], Train Loss: 0.0136, Train R2 Loss: 0.1640, Val Loss: 1.1954, R2 Val : 0.8360  Time for epoch 204: 1.68 seconds\n",
      "Epoch [205/500], Train Loss: 0.1288, Train R2 Loss: 0.2597, Val Loss: 1.8390, R2 Val : 0.7403  Time for epoch 205: 1.68 seconds\n",
      "Epoch [206/500], Train Loss: 0.0073, Train R2 Loss: 0.1322, Val Loss: 1.5079, R2 Val : 0.8678  Time for epoch 206: 1.67 seconds\n",
      "Epoch [207/500], Train Loss: 0.0678, Train R2 Loss: 0.1527, Val Loss: 1.5658, R2 Val : 0.8473  Time for epoch 207: 1.67 seconds\n",
      "Epoch [208/500], Train Loss: 0.0598, Train R2 Loss: 0.2156, Val Loss: 1.3788, R2 Val : 0.7844  Time for epoch 208: 1.67 seconds\n",
      "Epoch [209/500], Train Loss: 0.0904, Train R2 Loss: 0.1075, Val Loss: 1.6711, R2 Val : 0.8925  Time for epoch 209: 1.69 seconds\n",
      "Epoch [210/500], Train Loss: 0.0007, Train R2 Loss: 0.1878, Val Loss: 1.7401, R2 Val : 0.8122  Time for epoch 210: 1.67 seconds\n",
      "Epoch [211/500], Train Loss: 0.0201, Train R2 Loss: 0.0107, Val Loss: 1.6563, R2 Val : 0.9893  Time for epoch 211: 1.67 seconds\n",
      "Epoch [212/500], Train Loss: 0.7247, Train R2 Loss: 0.6315, Val Loss: 1.4531, R2 Val : 0.3685  Time for epoch 212: 1.68 seconds\n",
      "Epoch [213/500], Train Loss: 0.2417, Train R2 Loss: 0.1259, Val Loss: 2.1205, R2 Val : 0.8741  Time for epoch 213: 1.79 seconds\n",
      "Epoch [214/500], Train Loss: 0.1251, Train R2 Loss: 0.1172, Val Loss: 2.1847, R2 Val : 0.8828  Time for epoch 214: 1.74 seconds\n",
      "Epoch [215/500], Train Loss: 0.1417, Train R2 Loss: 0.0800, Val Loss: 2.1088, R2 Val : 0.9200  Time for epoch 215: 1.69 seconds\n",
      "Epoch [216/500], Train Loss: 0.0599, Train R2 Loss: 0.1191, Val Loss: 1.4483, R2 Val : 0.8809  Time for epoch 216: 1.68 seconds\n",
      "Epoch [217/500], Train Loss: 0.0610, Train R2 Loss: 0.3825, Val Loss: 1.9729, R2 Val : 0.6175  Time for epoch 217: 1.68 seconds\n",
      "Epoch [218/500], Train Loss: 0.2907, Train R2 Loss: 0.2401, Val Loss: 1.7385, R2 Val : 0.7599  Time for epoch 218: 1.77 seconds\n",
      "Epoch [219/500], Train Loss: 0.3455, Train R2 Loss: 0.2869, Val Loss: 1.2715, R2 Val : 0.7131  Time for epoch 219: 1.77 seconds\n",
      "Epoch [220/500], Train Loss: 0.0660, Train R2 Loss: 0.2439, Val Loss: 2.0026, R2 Val : 0.7561  Time for epoch 220: 1.79 seconds\n",
      "Epoch [221/500], Train Loss: 0.0397, Train R2 Loss: 0.1638, Val Loss: 2.1418, R2 Val : 0.8362  Time for epoch 221: 1.70 seconds\n",
      "Epoch [222/500], Train Loss: 0.0594, Train R2 Loss: 0.3377, Val Loss: 1.9391, R2 Val : 0.6623  Time for epoch 222: 1.70 seconds\n",
      "Epoch [223/500], Train Loss: 0.2440, Train R2 Loss: 0.1209, Val Loss: 1.3329, R2 Val : 0.8791  Time for epoch 223: 1.67 seconds\n",
      "Epoch [224/500], Train Loss: 0.1836, Train R2 Loss: 0.2321, Val Loss: 2.0518, R2 Val : 0.7679  Time for epoch 224: 1.68 seconds\n",
      "Epoch [225/500], Train Loss: 0.0267, Train R2 Loss: 0.2256, Val Loss: 1.6301, R2 Val : 0.7744  Time for epoch 225: 1.68 seconds\n",
      "Epoch [226/500], Train Loss: 0.0517, Train R2 Loss: 0.0645, Val Loss: 1.2727, R2 Val : 0.9355  Time for epoch 226: 1.69 seconds\n",
      "Epoch [227/500], Train Loss: 0.1451, Train R2 Loss: 0.0986, Val Loss: 1.9341, R2 Val : 0.9014  Time for epoch 227: 1.76 seconds\n",
      "Epoch [228/500], Train Loss: 0.1266, Train R2 Loss: 0.0991, Val Loss: 1.7824, R2 Val : 0.9009  Time for epoch 228: 1.67 seconds\n",
      "Epoch [229/500], Train Loss: 0.2261, Train R2 Loss: 0.1755, Val Loss: 1.7945, R2 Val : 0.8245  Time for epoch 229: 1.68 seconds\n",
      "Epoch [230/500], Train Loss: 0.0183, Train R2 Loss: 0.2987, Val Loss: 1.6403, R2 Val : 0.7013  Time for epoch 230: 1.69 seconds\n",
      "Epoch [231/500], Train Loss: 0.0553, Train R2 Loss: 0.2917, Val Loss: 1.7844, R2 Val : 0.7083  Time for epoch 231: 1.68 seconds\n",
      "Epoch [232/500], Train Loss: 0.1764, Train R2 Loss: 0.4502, Val Loss: 2.6523, R2 Val : 0.5498  Time for epoch 232: 1.68 seconds\n",
      "Epoch [233/500], Train Loss: 0.0716, Train R2 Loss: 0.2030, Val Loss: 1.2976, R2 Val : 0.7970  Time for epoch 233: 1.67 seconds\n",
      "Epoch [234/500], Train Loss: 0.1435, Train R2 Loss: 0.1164, Val Loss: 1.4107, R2 Val : 0.8836  Time for epoch 234: 1.68 seconds\n",
      "Epoch [235/500], Train Loss: 0.0215, Train R2 Loss: 0.1815, Val Loss: 2.2539, R2 Val : 0.8185  Time for epoch 235: 1.67 seconds\n",
      "Epoch [236/500], Train Loss: 0.0863, Train R2 Loss: 0.0589, Val Loss: 1.5070, R2 Val : 0.9411  Time for epoch 236: 1.67 seconds\n",
      "Epoch [237/500], Train Loss: 0.2396, Train R2 Loss: 0.1901, Val Loss: 1.7765, R2 Val : 0.8099  Time for epoch 237: 1.68 seconds\n",
      "Epoch [238/500], Train Loss: 0.2319, Train R2 Loss: 0.1218, Val Loss: 1.6896, R2 Val : 0.8782  Time for epoch 238: 1.68 seconds\n",
      "Epoch [239/500], Train Loss: 0.1137, Train R2 Loss: 0.2742, Val Loss: 2.0715, R2 Val : 0.7258  Time for epoch 239: 1.75 seconds\n",
      "Epoch [240/500], Train Loss: 0.1757, Train R2 Loss: 0.3325, Val Loss: 1.8461, R2 Val : 0.6675  Time for epoch 240: 1.68 seconds\n",
      "Epoch [241/500], Train Loss: 0.0048, Train R2 Loss: 0.1002, Val Loss: 1.1438, R2 Val : 0.8998  Time for epoch 241: 1.67 seconds\n",
      "Epoch [242/500], Train Loss: 0.0684, Train R2 Loss: 0.0725, Val Loss: 1.1774, R2 Val : 0.9275  Time for epoch 242: 1.67 seconds\n",
      "Epoch [243/500], Train Loss: 0.0205, Train R2 Loss: 0.0332, Val Loss: 1.0084, R2 Val : 0.9668  Time for epoch 243: 1.69 seconds\n",
      "Epoch [244/500], Train Loss: 0.0647, Train R2 Loss: 0.0848, Val Loss: 1.1317, R2 Val : 0.9152  Time for epoch 244: 1.71 seconds\n",
      "Epoch [245/500], Train Loss: 0.1425, Train R2 Loss: 0.0709, Val Loss: 1.8187, R2 Val : 0.9291  Time for epoch 245: 1.70 seconds\n",
      "Epoch [246/500], Train Loss: 0.2150, Train R2 Loss: 0.2043, Val Loss: 1.7239, R2 Val : 0.7957  Time for epoch 246: 1.68 seconds\n",
      "Epoch [247/500], Train Loss: 0.1214, Train R2 Loss: 0.1531, Val Loss: 1.4130, R2 Val : 0.8469  Time for epoch 247: 1.69 seconds\n",
      "Epoch [248/500], Train Loss: 0.1467, Train R2 Loss: 0.0866, Val Loss: 1.3792, R2 Val : 0.9134  Time for epoch 248: 1.67 seconds\n",
      "Epoch [249/500], Train Loss: 0.0222, Train R2 Loss: 0.0478, Val Loss: 1.3027, R2 Val : 0.9522  Time for epoch 249: 1.71 seconds\n",
      "Epoch [250/500], Train Loss: 0.0955, Train R2 Loss: 0.0534, Val Loss: 1.4382, R2 Val : 0.9466  Time for epoch 250: 1.68 seconds\n",
      "Epoch [251/500], Train Loss: 0.1689, Train R2 Loss: 0.3041, Val Loss: 2.5024, R2 Val : 0.6959  Time for epoch 251: 1.69 seconds\n",
      "Epoch [252/500], Train Loss: 0.7040, Train R2 Loss: 0.7153, Val Loss: 1.7244, R2 Val : 0.2847  Time for epoch 252: 1.71 seconds\n",
      "Epoch [253/500], Train Loss: 0.0364, Train R2 Loss: 0.0567, Val Loss: 1.6829, R2 Val : 0.9433  Time for epoch 253: 1.68 seconds\n",
      "Epoch [254/500], Train Loss: 0.4395, Train R2 Loss: 0.5299, Val Loss: 1.8525, R2 Val : 0.4701  Time for epoch 254: 1.69 seconds\n",
      "Epoch [255/500], Train Loss: 0.0181, Train R2 Loss: 0.1078, Val Loss: 1.4279, R2 Val : 0.8922  Time for epoch 255: 1.70 seconds\n",
      "Epoch [256/500], Train Loss: 0.0126, Train R2 Loss: 0.0514, Val Loss: 1.5287, R2 Val : 0.9486  Time for epoch 256: 1.72 seconds\n",
      "Epoch [257/500], Train Loss: 0.0313, Train R2 Loss: 0.1025, Val Loss: 1.5530, R2 Val : 0.8975  Time for epoch 257: 1.70 seconds\n",
      "Epoch [258/500], Train Loss: 0.0707, Train R2 Loss: 0.0450, Val Loss: 1.6133, R2 Val : 0.9550  Time for epoch 258: 1.70 seconds\n",
      "Epoch [259/500], Train Loss: 0.0265, Train R2 Loss: 0.0942, Val Loss: 1.6682, R2 Val : 0.9058  Time for epoch 259: 1.69 seconds\n",
      "Epoch [260/500], Train Loss: 0.2424, Train R2 Loss: 0.3313, Val Loss: 2.1647, R2 Val : 0.6687  Time for epoch 260: 1.76 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [261/500], Train Loss: 0.3390, Train R2 Loss: 0.1957, Val Loss: 1.4478, R2 Val : 0.8043  Time for epoch 261: 1.84 seconds\n",
      "Epoch [262/500], Train Loss: 0.0031, Train R2 Loss: 0.1317, Val Loss: 1.4286, R2 Val : 0.8683  Time for epoch 262: 1.75 seconds\n",
      "Epoch [263/500], Train Loss: 0.1958, Train R2 Loss: 0.3100, Val Loss: 1.9069, R2 Val : 0.6900  Time for epoch 263: 1.70 seconds\n",
      "Epoch [264/500], Train Loss: 0.0178, Train R2 Loss: 0.0388, Val Loss: 1.2905, R2 Val : 0.9612  Time for epoch 264: 1.69 seconds\n",
      "Epoch [265/500], Train Loss: 0.0535, Train R2 Loss: 0.0909, Val Loss: 1.3689, R2 Val : 0.9091  Time for epoch 265: 1.74 seconds\n",
      "Epoch [266/500], Train Loss: 0.0872, Train R2 Loss: 0.0486, Val Loss: 1.3982, R2 Val : 0.9514  Time for epoch 266: 1.73 seconds\n",
      "Epoch [267/500], Train Loss: 0.0345, Train R2 Loss: 0.2144, Val Loss: 2.5819, R2 Val : 0.7856  Time for epoch 267: 1.70 seconds\n",
      "Epoch [268/500], Train Loss: 0.0401, Train R2 Loss: 0.0801, Val Loss: 1.2874, R2 Val : 0.9199  Time for epoch 268: 1.75 seconds\n",
      "Epoch [269/500], Train Loss: 0.0255, Train R2 Loss: 0.0873, Val Loss: 1.5234, R2 Val : 0.9127  Time for epoch 269: 1.75 seconds\n",
      "Epoch [270/500], Train Loss: 0.0680, Train R2 Loss: 0.1501, Val Loss: 1.7980, R2 Val : 0.8499  Time for epoch 270: 1.73 seconds\n",
      "Epoch [271/500], Train Loss: 0.0993, Train R2 Loss: 0.1771, Val Loss: 1.8168, R2 Val : 0.8229  Time for epoch 271: 1.76 seconds\n",
      "Epoch [272/500], Train Loss: 0.0909, Train R2 Loss: 0.1756, Val Loss: 1.6624, R2 Val : 0.8244  Time for epoch 272: 1.77 seconds\n",
      "Epoch [273/500], Train Loss: 0.0044, Train R2 Loss: 0.0552, Val Loss: 1.5351, R2 Val : 0.9448  Time for epoch 273: 1.70 seconds\n",
      "Epoch [274/500], Train Loss: 0.3746, Train R2 Loss: 0.4232, Val Loss: 1.6102, R2 Val : 0.5768  Time for epoch 274: 1.76 seconds\n",
      "Epoch [275/500], Train Loss: 0.1267, Train R2 Loss: 0.1565, Val Loss: 1.6694, R2 Val : 0.8435  Time for epoch 275: 1.74 seconds\n",
      "Epoch [276/500], Train Loss: 0.1124, Train R2 Loss: 0.0314, Val Loss: 1.5586, R2 Val : 0.9686  Time for epoch 276: 1.74 seconds\n",
      "Epoch [277/500], Train Loss: 0.2438, Train R2 Loss: 0.2883, Val Loss: 1.8285, R2 Val : 0.7117  Time for epoch 277: 1.68 seconds\n",
      "Epoch [278/500], Train Loss: 0.0654, Train R2 Loss: 0.1582, Val Loss: 1.5813, R2 Val : 0.8418  Time for epoch 278: 1.72 seconds\n",
      "Epoch [279/500], Train Loss: 0.0489, Train R2 Loss: 0.2041, Val Loss: 1.7074, R2 Val : 0.7959  Time for epoch 279: 1.79 seconds\n",
      "Epoch [280/500], Train Loss: 0.0173, Train R2 Loss: 0.1323, Val Loss: 1.3942, R2 Val : 0.8677  Time for epoch 280: 1.69 seconds\n",
      "Epoch [281/500], Train Loss: 0.2190, Train R2 Loss: 0.1035, Val Loss: 1.4560, R2 Val : 0.8965  Time for epoch 281: 1.68 seconds\n",
      "Epoch [282/500], Train Loss: 0.0740, Train R2 Loss: 0.0469, Val Loss: 1.2304, R2 Val : 0.9531  Time for epoch 282: 1.69 seconds\n",
      "Epoch [283/500], Train Loss: 0.2128, Train R2 Loss: 0.0947, Val Loss: 1.9928, R2 Val : 0.9053  Time for epoch 283: 1.68 seconds\n",
      "Epoch [284/500], Train Loss: 0.0048, Train R2 Loss: 0.0758, Val Loss: 1.6084, R2 Val : 0.9242  Time for epoch 284: 1.68 seconds\n",
      "Epoch [285/500], Train Loss: 0.1741, Train R2 Loss: 0.1860, Val Loss: 1.5288, R2 Val : 0.8140  Time for epoch 285: 1.68 seconds\n",
      "Epoch [286/500], Train Loss: 0.0194, Train R2 Loss: 0.0658, Val Loss: 1.3985, R2 Val : 0.9342  Time for epoch 286: 1.68 seconds\n",
      "Epoch [287/500], Train Loss: 0.0820, Train R2 Loss: 0.1530, Val Loss: 2.0764, R2 Val : 0.8470  Time for epoch 287: 1.71 seconds\n",
      "Epoch [288/500], Train Loss: 0.0416, Train R2 Loss: 0.0609, Val Loss: 1.4091, R2 Val : 0.9391  Time for epoch 288: 1.67 seconds\n",
      "Epoch [289/500], Train Loss: 0.1238, Train R2 Loss: 0.4018, Val Loss: 2.4382, R2 Val : 0.5982  Time for epoch 289: 1.67 seconds\n",
      "Epoch [290/500], Train Loss: 0.1298, Train R2 Loss: 0.2848, Val Loss: 2.0646, R2 Val : 0.7152  Time for epoch 290: 1.68 seconds\n",
      "Epoch [291/500], Train Loss: 0.0011, Train R2 Loss: 0.0916, Val Loss: 1.5107, R2 Val : 0.9084  Time for epoch 291: 1.67 seconds\n",
      "Epoch [292/500], Train Loss: 0.0987, Train R2 Loss: 0.0535, Val Loss: 1.3177, R2 Val : 0.9465  Time for epoch 292: 1.67 seconds\n",
      "Epoch [293/500], Train Loss: 0.0223, Train R2 Loss: 0.0575, Val Loss: 1.3114, R2 Val : 0.9425  Time for epoch 293: 1.69 seconds\n",
      "Epoch [294/500], Train Loss: 0.0345, Train R2 Loss: 0.0534, Val Loss: 1.5285, R2 Val : 0.9466  Time for epoch 294: 1.73 seconds\n",
      "Epoch [295/500], Train Loss: 0.0008, Train R2 Loss: 0.0976, Val Loss: 1.6837, R2 Val : 0.9024  Time for epoch 295: 1.81 seconds\n",
      "Epoch [296/500], Train Loss: 0.3975, Train R2 Loss: 0.3327, Val Loss: 1.5966, R2 Val : 0.6673  Time for epoch 296: 1.71 seconds\n",
      "Epoch [297/500], Train Loss: 0.0720, Train R2 Loss: 0.2313, Val Loss: 1.8658, R2 Val : 0.7687  Time for epoch 297: 1.69 seconds\n",
      "Epoch [298/500], Train Loss: 0.1366, Train R2 Loss: 0.1085, Val Loss: 1.3862, R2 Val : 0.8915  Time for epoch 298: 1.69 seconds\n",
      "Epoch [299/500], Train Loss: 0.0626, Train R2 Loss: 0.0799, Val Loss: 1.3473, R2 Val : 0.9201  Time for epoch 299: 1.69 seconds\n",
      "Epoch [300/500], Train Loss: 0.0353, Train R2 Loss: 0.0761, Val Loss: 1.3975, R2 Val : 0.9239  Time for epoch 300: 1.69 seconds\n",
      "Epoch [301/500], Train Loss: 0.2238, Train R2 Loss: 0.1956, Val Loss: 1.6002, R2 Val : 0.8044  Time for epoch 301: 1.70 seconds\n",
      "Epoch [302/500], Train Loss: 0.0404, Train R2 Loss: 0.0498, Val Loss: 1.2809, R2 Val : 0.9502  Time for epoch 302: 1.69 seconds\n",
      "Epoch [303/500], Train Loss: 0.1279, Train R2 Loss: 0.1295, Val Loss: 1.3399, R2 Val : 0.8705  Time for epoch 303: 1.78 seconds\n",
      "Epoch [304/500], Train Loss: 0.0921, Train R2 Loss: 0.0571, Val Loss: 1.3635, R2 Val : 0.9429  Time for epoch 304: 1.75 seconds\n",
      "Epoch [305/500], Train Loss: 0.0181, Train R2 Loss: 0.2186, Val Loss: 1.5362, R2 Val : 0.7814  Time for epoch 305: 1.71 seconds\n",
      "Epoch [306/500], Train Loss: 0.2185, Train R2 Loss: 0.2983, Val Loss: 1.5506, R2 Val : 0.7017  Time for epoch 306: 1.69 seconds\n",
      "Epoch [307/500], Train Loss: 0.0527, Train R2 Loss: 0.0989, Val Loss: 1.4907, R2 Val : 0.9011  Time for epoch 307: 1.68 seconds\n",
      "Epoch [308/500], Train Loss: 0.3468, Train R2 Loss: 0.0572, Val Loss: 1.2282, R2 Val : 0.9428  Time for epoch 308: 1.68 seconds\n",
      "Epoch [309/500], Train Loss: 0.0904, Train R2 Loss: 0.1761, Val Loss: 1.4180, R2 Val : 0.8239  Time for epoch 309: 1.73 seconds\n",
      "Epoch [310/500], Train Loss: 0.0491, Train R2 Loss: 0.1119, Val Loss: 1.2095, R2 Val : 0.8881  Time for epoch 310: 1.76 seconds\n",
      "Epoch [311/500], Train Loss: 0.0666, Train R2 Loss: 0.1055, Val Loss: 1.3148, R2 Val : 0.8945  Time for epoch 311: 1.69 seconds\n",
      "Epoch [312/500], Train Loss: 0.0057, Train R2 Loss: 0.0112, Val Loss: 1.4126, R2 Val : 0.9888  Time for epoch 312: 1.78 seconds\n",
      "Epoch [313/500], Train Loss: 0.0008, Train R2 Loss: 0.1202, Val Loss: 1.2289, R2 Val : 0.8798  Time for epoch 313: 1.75 seconds\n",
      "Epoch [314/500], Train Loss: 0.0006, Train R2 Loss: 0.0645, Val Loss: 1.4033, R2 Val : 0.9355  Time for epoch 314: 1.76 seconds\n",
      "Epoch [315/500], Train Loss: 0.2693, Train R2 Loss: 0.2020, Val Loss: 1.4792, R2 Val : 0.7980  Time for epoch 315: 1.69 seconds\n",
      "Epoch [316/500], Train Loss: 0.0754, Train R2 Loss: 0.1683, Val Loss: 1.6757, R2 Val : 0.8317  Time for epoch 316: 1.68 seconds\n",
      "Epoch [317/500], Train Loss: 0.0060, Train R2 Loss: 0.0337, Val Loss: 1.2176, R2 Val : 0.9663  Time for epoch 317: 1.68 seconds\n",
      "Epoch [318/500], Train Loss: 0.2138, Train R2 Loss: 0.0814, Val Loss: 1.2075, R2 Val : 0.9186  Time for epoch 318: 1.69 seconds\n",
      "Epoch [319/500], Train Loss: 0.0376, Train R2 Loss: 0.0534, Val Loss: 1.1146, R2 Val : 0.9466  Time for epoch 319: 1.68 seconds\n",
      "Epoch [320/500], Train Loss: 0.0648, Train R2 Loss: 0.0472, Val Loss: 1.3877, R2 Val : 0.9528  Time for epoch 320: 1.69 seconds\n",
      "Epoch [321/500], Train Loss: 0.0534, Train R2 Loss: 0.0630, Val Loss: 1.1796, R2 Val : 0.9370  Time for epoch 321: 1.68 seconds\n",
      "Epoch [322/500], Train Loss: 0.0295, Train R2 Loss: 0.1743, Val Loss: 1.4060, R2 Val : 0.8257  Time for epoch 322: 1.67 seconds\n",
      "Epoch [323/500], Train Loss: 0.1021, Train R2 Loss: 0.0281, Val Loss: 1.4163, R2 Val : 0.9719  Time for epoch 323: 1.68 seconds\n",
      "Epoch [324/500], Train Loss: 0.0035, Train R2 Loss: 0.0892, Val Loss: 1.4564, R2 Val : 0.9108  Time for epoch 324: 1.72 seconds\n",
      "Epoch [325/500], Train Loss: 0.1778, Train R2 Loss: 0.2795, Val Loss: 1.8990, R2 Val : 0.7205  Time for epoch 325: 1.68 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [326/500], Train Loss: 0.1387, Train R2 Loss: 0.1617, Val Loss: 1.3453, R2 Val : 0.8383  Time for epoch 326: 1.68 seconds\n",
      "Epoch [327/500], Train Loss: 0.1367, Train R2 Loss: 0.0860, Val Loss: 1.7048, R2 Val : 0.9140  Time for epoch 327: 1.68 seconds\n",
      "Epoch [328/500], Train Loss: 0.3262, Train R2 Loss: 0.1346, Val Loss: 1.2458, R2 Val : 0.8654  Time for epoch 328: 1.67 seconds\n",
      "Epoch [329/500], Train Loss: 0.3096, Train R2 Loss: 0.1396, Val Loss: 1.3066, R2 Val : 0.8604  Time for epoch 329: 1.68 seconds\n",
      "Epoch [330/500], Train Loss: 0.0603, Train R2 Loss: 0.1172, Val Loss: 1.5295, R2 Val : 0.8828  Time for epoch 330: 1.68 seconds\n",
      "Epoch [331/500], Train Loss: 0.1066, Train R2 Loss: 0.1188, Val Loss: 1.2446, R2 Val : 0.8812  Time for epoch 331: 1.68 seconds\n",
      "Epoch [332/500], Train Loss: 0.1458, Train R2 Loss: 0.1420, Val Loss: 1.6291, R2 Val : 0.8580  Time for epoch 332: 1.68 seconds\n",
      "Epoch [333/500], Train Loss: 0.0268, Train R2 Loss: 0.1170, Val Loss: 1.3498, R2 Val : 0.8830  Time for epoch 333: 1.68 seconds\n",
      "Epoch [334/500], Train Loss: 0.0469, Train R2 Loss: 0.1292, Val Loss: 1.4380, R2 Val : 0.8708  Time for epoch 334: 1.68 seconds\n",
      "Epoch [335/500], Train Loss: 0.0559, Train R2 Loss: 0.1288, Val Loss: 1.2022, R2 Val : 0.8712  Time for epoch 335: 1.68 seconds\n",
      "Epoch [336/500], Train Loss: 0.1359, Train R2 Loss: 0.0763, Val Loss: 1.5424, R2 Val : 0.9237  Time for epoch 336: 1.68 seconds\n",
      "Epoch [337/500], Train Loss: 0.0385, Train R2 Loss: 0.1005, Val Loss: 1.2329, R2 Val : 0.8995  Time for epoch 337: 1.68 seconds\n",
      "Epoch [338/500], Train Loss: 0.1920, Train R2 Loss: 0.1385, Val Loss: 1.3360, R2 Val : 0.8615  Time for epoch 338: 1.67 seconds\n",
      "Epoch [339/500], Train Loss: 0.6814, Train R2 Loss: 0.5654, Val Loss: 1.1908, R2 Val : 0.4346  Time for epoch 339: 1.67 seconds\n",
      "Epoch [340/500], Train Loss: 0.0658, Train R2 Loss: 0.0645, Val Loss: 1.1055, R2 Val : 0.9355  Time for epoch 340: 1.69 seconds\n",
      "Epoch [341/500], Train Loss: 0.0012, Train R2 Loss: 0.0314, Val Loss: 1.2440, R2 Val : 0.9686  Time for epoch 341: 1.71 seconds\n",
      "Epoch [342/500], Train Loss: 0.2293, Train R2 Loss: 0.1174, Val Loss: 1.2333, R2 Val : 0.8826  Time for epoch 342: 1.71 seconds\n",
      "Epoch [343/500], Train Loss: 0.0602, Train R2 Loss: 0.1251, Val Loss: 1.3824, R2 Val : 0.8749  Time for epoch 343: 1.70 seconds\n",
      "Epoch [344/500], Train Loss: 0.1840, Train R2 Loss: 0.0303, Val Loss: 1.1954, R2 Val : 0.9697  Time for epoch 344: 1.67 seconds\n",
      "Epoch [345/500], Train Loss: 0.0690, Train R2 Loss: 0.0617, Val Loss: 1.3484, R2 Val : 0.9383  Time for epoch 345: 1.69 seconds\n",
      "Epoch [346/500], Train Loss: 0.1651, Train R2 Loss: 0.0125, Val Loss: 1.3380, R2 Val : 0.9875  Time for epoch 346: 1.71 seconds\n",
      "Epoch [347/500], Train Loss: 0.0968, Train R2 Loss: 0.1189, Val Loss: 1.0938, R2 Val : 0.8811  Time for epoch 347: 1.72 seconds\n",
      "Epoch [348/500], Train Loss: 0.1440, Train R2 Loss: 0.1273, Val Loss: 1.3206, R2 Val : 0.8727  Time for epoch 348: 1.69 seconds\n",
      "Epoch [349/500], Train Loss: 0.0371, Train R2 Loss: 0.1231, Val Loss: 1.3134, R2 Val : 0.8769  Time for epoch 349: 1.71 seconds\n",
      "Epoch [350/500], Train Loss: 0.0033, Train R2 Loss: 0.0499, Val Loss: 1.1850, R2 Val : 0.9501  Time for epoch 350: 1.68 seconds\n",
      "Epoch [351/500], Train Loss: 0.0160, Train R2 Loss: 0.0771, Val Loss: 1.1040, R2 Val : 0.9229  Time for epoch 351: 1.72 seconds\n",
      "Epoch [352/500], Train Loss: 0.0412, Train R2 Loss: 0.0469, Val Loss: 1.1132, R2 Val : 0.9531  Time for epoch 352: 1.71 seconds\n",
      "Epoch [353/500], Train Loss: 0.0159, Train R2 Loss: 0.1005, Val Loss: 1.9719, R2 Val : 0.8995  Time for epoch 353: 1.69 seconds\n",
      "Epoch [354/500], Train Loss: 0.0559, Train R2 Loss: 0.0625, Val Loss: 1.0654, R2 Val : 0.9375  Time for epoch 354: 1.78 seconds\n",
      "Epoch [355/500], Train Loss: 0.1067, Train R2 Loss: 0.0577, Val Loss: 1.2379, R2 Val : 0.9423  Time for epoch 355: 1.73 seconds\n",
      "Epoch [356/500], Train Loss: 0.0892, Train R2 Loss: 0.1245, Val Loss: 1.9629, R2 Val : 0.8755  Time for epoch 356: 1.69 seconds\n",
      "Epoch [357/500], Train Loss: 0.0964, Train R2 Loss: 0.0327, Val Loss: 1.0651, R2 Val : 0.9673  Time for epoch 357: 1.68 seconds\n",
      "Epoch [358/500], Train Loss: 0.0975, Train R2 Loss: 0.1098, Val Loss: 1.4462, R2 Val : 0.8902  Time for epoch 358: 1.68 seconds\n",
      "Epoch [359/500], Train Loss: 0.0170, Train R2 Loss: 0.0968, Val Loss: 1.6403, R2 Val : 0.9032  Time for epoch 359: 1.69 seconds\n",
      "Epoch [360/500], Train Loss: 0.0177, Train R2 Loss: 0.0895, Val Loss: 1.0778, R2 Val : 0.9105  Time for epoch 360: 1.71 seconds\n",
      "Epoch [361/500], Train Loss: 0.0027, Train R2 Loss: 0.0491, Val Loss: 0.9228, R2 Val : 0.9509  Time for epoch 361: 1.76 seconds\n",
      "Epoch [362/500], Train Loss: 0.0147, Train R2 Loss: 0.0670, Val Loss: 1.0680, R2 Val : 0.9330  Time for epoch 362: 1.71 seconds\n",
      "Epoch [363/500], Train Loss: 0.0304, Train R2 Loss: 0.0303, Val Loss: 1.0112, R2 Val : 0.9697  Time for epoch 363: 1.72 seconds\n",
      "Epoch [364/500], Train Loss: 0.2551, Train R2 Loss: 0.1626, Val Loss: 0.9837, R2 Val : 0.8374  Time for epoch 364: 1.68 seconds\n",
      "Epoch [365/500], Train Loss: 0.0029, Train R2 Loss: 0.0517, Val Loss: 0.9654, R2 Val : 0.9483  Time for epoch 365: 1.71 seconds\n",
      "Epoch [366/500], Train Loss: 0.1927, Train R2 Loss: 0.2593, Val Loss: 1.7463, R2 Val : 0.7407  Time for epoch 366: 1.68 seconds\n",
      "Epoch [367/500], Train Loss: 0.1368, Train R2 Loss: 0.0932, Val Loss: 1.2133, R2 Val : 0.9068  Time for epoch 367: 1.68 seconds\n",
      "Epoch [368/500], Train Loss: 0.0777, Train R2 Loss: 0.1597, Val Loss: 1.7800, R2 Val : 0.8403  Time for epoch 368: 1.68 seconds\n",
      "Epoch [369/500], Train Loss: 0.1936, Train R2 Loss: 0.1618, Val Loss: 1.6575, R2 Val : 0.8382  Time for epoch 369: 1.67 seconds\n",
      "Epoch [370/500], Train Loss: 0.0702, Train R2 Loss: 0.0409, Val Loss: 0.9559, R2 Val : 0.9591  Time for epoch 370: 1.67 seconds\n",
      "Epoch [371/500], Train Loss: 0.0457, Train R2 Loss: 0.0783, Val Loss: 1.4846, R2 Val : 0.9217  Time for epoch 371: 1.68 seconds\n",
      "Epoch [372/500], Train Loss: 0.2675, Train R2 Loss: 0.2670, Val Loss: 2.0335, R2 Val : 0.7330  Time for epoch 372: 1.68 seconds\n",
      "Epoch [373/500], Train Loss: 0.1797, Train R2 Loss: 0.2731, Val Loss: 2.3710, R2 Val : 0.7269  Time for epoch 373: 1.68 seconds\n",
      "Epoch [374/500], Train Loss: 0.1849, Train R2 Loss: 0.1067, Val Loss: 0.9566, R2 Val : 0.8933  Time for epoch 374: 1.70 seconds\n",
      "Epoch [375/500], Train Loss: 0.0890, Train R2 Loss: 0.0098, Val Loss: 1.4290, R2 Val : 0.9902  Time for epoch 375: 1.76 seconds\n",
      "Epoch [376/500], Train Loss: 0.1130, Train R2 Loss: 0.0630, Val Loss: 1.6352, R2 Val : 0.9370  Time for epoch 376: 1.69 seconds\n",
      "Epoch [377/500], Train Loss: 2.3295, Train R2 Loss: 0.4761, Val Loss: 1.9123, R2 Val : 0.5239  Time for epoch 377: 1.67 seconds\n",
      "Epoch [378/500], Train Loss: 0.1936, Train R2 Loss: 0.1006, Val Loss: 1.3471, R2 Val : 0.8994  Time for epoch 378: 1.68 seconds\n",
      "Epoch [379/500], Train Loss: 0.0133, Train R2 Loss: 0.0195, Val Loss: 0.9986, R2 Val : 0.9805  Time for epoch 379: 1.68 seconds\n",
      "Epoch [380/500], Train Loss: 0.0475, Train R2 Loss: 0.0251, Val Loss: 1.0632, R2 Val : 0.9749  Time for epoch 380: 1.67 seconds\n",
      "Epoch [381/500], Train Loss: 0.0008, Train R2 Loss: 0.0702, Val Loss: 1.1691, R2 Val : 0.9298  Time for epoch 381: 1.69 seconds\n",
      "Epoch [382/500], Train Loss: 0.0817, Train R2 Loss: 0.0417, Val Loss: 1.2320, R2 Val : 0.9583  Time for epoch 382: 1.73 seconds\n",
      "Epoch [383/500], Train Loss: 0.1399, Train R2 Loss: 0.0770, Val Loss: 1.2567, R2 Val : 0.9230  Time for epoch 383: 1.72 seconds\n",
      "Epoch [384/500], Train Loss: 0.0123, Train R2 Loss: 0.0657, Val Loss: 1.2540, R2 Val : 0.9343  Time for epoch 384: 1.68 seconds\n",
      "Epoch [385/500], Train Loss: 0.1148, Train R2 Loss: 0.0356, Val Loss: 1.2555, R2 Val : 0.9644  Time for epoch 385: 1.75 seconds\n",
      "Epoch [386/500], Train Loss: 0.0007, Train R2 Loss: 0.0399, Val Loss: 0.8730, R2 Val : 0.9601  Time for epoch 386: 1.67 seconds\n",
      "Epoch [387/500], Train Loss: 0.0311, Train R2 Loss: 0.0258, Val Loss: 0.9913, R2 Val : 0.9742  Time for epoch 387: 1.68 seconds\n",
      "Epoch [388/500], Train Loss: 0.0824, Train R2 Loss: 0.0873, Val Loss: 1.1968, R2 Val : 0.9127  Time for epoch 388: 1.70 seconds\n",
      "Epoch [389/500], Train Loss: 0.0861, Train R2 Loss: 0.0343, Val Loss: 0.9094, R2 Val : 0.9657  Time for epoch 389: 1.68 seconds\n",
      "Epoch [390/500], Train Loss: 0.1604, Train R2 Loss: 0.0625, Val Loss: 0.9678, R2 Val : 0.9375  Time for epoch 390: 1.68 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [391/500], Train Loss: 0.1563, Train R2 Loss: 0.0350, Val Loss: 1.0344, R2 Val : 0.9650  Time for epoch 391: 1.67 seconds\n",
      "Epoch [392/500], Train Loss: 0.0567, Train R2 Loss: 0.0234, Val Loss: 0.8866, R2 Val : 0.9766  Time for epoch 392: 1.67 seconds\n",
      "Epoch [393/500], Train Loss: 0.0442, Train R2 Loss: 0.0598, Val Loss: 1.0409, R2 Val : 0.9402  Time for epoch 393: 1.68 seconds\n",
      "Epoch [394/500], Train Loss: 0.1280, Train R2 Loss: 0.0448, Val Loss: 1.0259, R2 Val : 0.9552  Time for epoch 394: 1.69 seconds\n",
      "Epoch [395/500], Train Loss: 0.0371, Train R2 Loss: 0.0697, Val Loss: 0.9640, R2 Val : 0.9303  Time for epoch 395: 1.68 seconds\n",
      "Epoch [396/500], Train Loss: 0.1105, Train R2 Loss: 0.0703, Val Loss: 1.8096, R2 Val : 0.9297  Time for epoch 396: 1.73 seconds\n",
      "Epoch [397/500], Train Loss: 0.0433, Train R2 Loss: 0.0521, Val Loss: 1.1225, R2 Val : 0.9479  Time for epoch 397: 1.73 seconds\n",
      "Epoch [398/500], Train Loss: 0.0513, Train R2 Loss: 0.0474, Val Loss: 1.0746, R2 Val : 0.9526  Time for epoch 398: 1.72 seconds\n",
      "Epoch [399/500], Train Loss: 0.0438, Train R2 Loss: 0.0574, Val Loss: 0.9061, R2 Val : 0.9426  Time for epoch 399: 1.70 seconds\n",
      "Epoch [400/500], Train Loss: 0.1082, Train R2 Loss: 0.0463, Val Loss: 0.7109, R2 Val : 0.9537  Time for epoch 400: 1.68 seconds\n",
      "Epoch [401/500], Train Loss: 0.0135, Train R2 Loss: 0.0967, Val Loss: 1.4123, R2 Val : 0.9033  Time for epoch 401: 1.69 seconds\n",
      "Epoch [402/500], Train Loss: 0.0027, Train R2 Loss: 0.0317, Val Loss: 1.0898, R2 Val : 0.9683  Time for epoch 402: 1.77 seconds\n",
      "Epoch [403/500], Train Loss: 0.0893, Train R2 Loss: 0.0649, Val Loss: 1.5015, R2 Val : 0.9351  Time for epoch 403: 1.68 seconds\n",
      "Epoch [404/500], Train Loss: 0.0053, Train R2 Loss: 0.0174, Val Loss: 0.6928, R2 Val : 0.9826  Time for epoch 404: 1.68 seconds\n",
      "Epoch [405/500], Train Loss: 0.0125, Train R2 Loss: 0.0398, Val Loss: 0.9056, R2 Val : 0.9602  Time for epoch 405: 1.70 seconds\n",
      "Epoch [406/500], Train Loss: 0.0445, Train R2 Loss: 0.1080, Val Loss: 2.3484, R2 Val : 0.8920  Time for epoch 406: 1.72 seconds\n",
      "Epoch [407/500], Train Loss: 0.1587, Train R2 Loss: 0.0567, Val Loss: 1.0543, R2 Val : 0.9433  Time for epoch 407: 1.75 seconds\n",
      "Epoch [408/500], Train Loss: 0.1020, Train R2 Loss: 0.0516, Val Loss: 1.2017, R2 Val : 0.9484  Time for epoch 408: 1.69 seconds\n",
      "Epoch [409/500], Train Loss: 0.1250, Train R2 Loss: 0.1330, Val Loss: 1.2160, R2 Val : 0.8670  Time for epoch 409: 1.68 seconds\n",
      "Epoch [410/500], Train Loss: 0.1206, Train R2 Loss: 0.1448, Val Loss: 1.3029, R2 Val : 0.8552  Time for epoch 410: 1.70 seconds\n",
      "Epoch [411/500], Train Loss: 0.0243, Train R2 Loss: 0.0691, Val Loss: 0.9683, R2 Val : 0.9309  Time for epoch 411: 1.74 seconds\n",
      "Epoch [412/500], Train Loss: 0.0538, Train R2 Loss: 0.0647, Val Loss: 0.8709, R2 Val : 0.9353  Time for epoch 412: 1.68 seconds\n",
      "Epoch [413/500], Train Loss: 0.0428, Train R2 Loss: 0.0235, Val Loss: 1.0719, R2 Val : 0.9765  Time for epoch 413: 1.67 seconds\n",
      "Epoch [414/500], Train Loss: 0.0694, Train R2 Loss: 0.0798, Val Loss: 0.9681, R2 Val : 0.9202  Time for epoch 414: 1.68 seconds\n",
      "Epoch [415/500], Train Loss: 0.1717, Train R2 Loss: 0.0407, Val Loss: 1.0188, R2 Val : 0.9593  Time for epoch 415: 1.70 seconds\n",
      "Epoch [416/500], Train Loss: 0.0481, Train R2 Loss: 0.0273, Val Loss: 0.7346, R2 Val : 0.9727  Time for epoch 416: 1.70 seconds\n",
      "Epoch [417/500], Train Loss: 0.0082, Train R2 Loss: 0.0123, Val Loss: 0.6428, R2 Val : 0.9877  Time for epoch 417: 1.70 seconds\n",
      "Epoch [418/500], Train Loss: 0.0425, Train R2 Loss: 0.0249, Val Loss: 1.0032, R2 Val : 0.9751  Time for epoch 418: 1.68 seconds\n",
      "Epoch [419/500], Train Loss: 0.0591, Train R2 Loss: 0.0697, Val Loss: 1.1721, R2 Val : 0.9303  Time for epoch 419: 1.70 seconds\n",
      "Epoch [420/500], Train Loss: 0.0042, Train R2 Loss: 0.0465, Val Loss: 1.4027, R2 Val : 0.9535  Time for epoch 420: 1.67 seconds\n",
      "Epoch [421/500], Train Loss: 0.0571, Train R2 Loss: 0.1019, Val Loss: 1.1210, R2 Val : 0.8981  Time for epoch 421: 1.68 seconds\n",
      "Epoch [422/500], Train Loss: 0.0357, Train R2 Loss: 0.0662, Val Loss: 1.2617, R2 Val : 0.9338  Time for epoch 422: 1.68 seconds\n",
      "Epoch [423/500], Train Loss: 0.2328, Train R2 Loss: 0.0316, Val Loss: 1.2478, R2 Val : 0.9684  Time for epoch 423: 1.68 seconds\n",
      "Epoch [424/500], Train Loss: 0.0053, Train R2 Loss: 0.0401, Val Loss: 0.9861, R2 Val : 0.9599  Time for epoch 424: 1.68 seconds\n",
      "Epoch [425/500], Train Loss: 0.0453, Train R2 Loss: 0.1101, Val Loss: 1.6466, R2 Val : 0.8899  Time for epoch 425: 1.68 seconds\n",
      "Epoch [426/500], Train Loss: 0.0252, Train R2 Loss: 0.0569, Val Loss: 1.1022, R2 Val : 0.9431  Time for epoch 426: 1.68 seconds\n",
      "Epoch [427/500], Train Loss: 0.0877, Train R2 Loss: 0.1442, Val Loss: 1.4462, R2 Val : 0.8558  Time for epoch 427: 1.67 seconds\n",
      "Epoch [428/500], Train Loss: 0.0359, Train R2 Loss: 0.0418, Val Loss: 0.8328, R2 Val : 0.9582  Time for epoch 428: 1.68 seconds\n",
      "Epoch [429/500], Train Loss: 0.1700, Train R2 Loss: 0.2353, Val Loss: 1.8604, R2 Val : 0.7647  Time for epoch 429: 1.68 seconds\n",
      "Epoch [430/500], Train Loss: 0.0011, Train R2 Loss: 0.0980, Val Loss: 1.0827, R2 Val : 0.9020  Time for epoch 430: 1.67 seconds\n",
      "Epoch [431/500], Train Loss: 0.0062, Train R2 Loss: 0.0489, Val Loss: 0.9871, R2 Val : 0.9511  Time for epoch 431: 1.67 seconds\n",
      "Epoch [432/500], Train Loss: 0.0796, Train R2 Loss: 0.0769, Val Loss: 1.4847, R2 Val : 0.9231  Time for epoch 432: 1.67 seconds\n",
      "Epoch [433/500], Train Loss: 0.0478, Train R2 Loss: 0.0494, Val Loss: 1.2244, R2 Val : 0.9506  Time for epoch 433: 1.68 seconds\n",
      "Epoch [434/500], Train Loss: 0.0026, Train R2 Loss: 0.0248, Val Loss: 1.0117, R2 Val : 0.9752  Time for epoch 434: 1.68 seconds\n",
      "Epoch [435/500], Train Loss: 0.0000, Train R2 Loss: 0.0130, Val Loss: 0.7387, R2 Val : 0.9870  Time for epoch 435: 1.67 seconds\n",
      "Epoch [436/500], Train Loss: 0.0973, Train R2 Loss: 0.0377, Val Loss: 1.0746, R2 Val : 0.9623  Time for epoch 436: 1.68 seconds\n",
      "Epoch [437/500], Train Loss: 0.0460, Train R2 Loss: 0.0262, Val Loss: 1.0381, R2 Val : 0.9738  Time for epoch 437: 1.67 seconds\n",
      "Epoch [438/500], Train Loss: 0.0478, Train R2 Loss: 0.0503, Val Loss: 0.9095, R2 Val : 0.9497  Time for epoch 438: 1.69 seconds\n",
      "Epoch [439/500], Train Loss: 0.0161, Train R2 Loss: 0.0667, Val Loss: 1.5657, R2 Val : 0.9333  Time for epoch 439: 1.68 seconds\n",
      "Epoch [440/500], Train Loss: 0.0191, Train R2 Loss: 0.0605, Val Loss: 1.1915, R2 Val : 0.9395  Time for epoch 440: 1.67 seconds\n",
      "Epoch [441/500], Train Loss: 0.0513, Train R2 Loss: 0.0857, Val Loss: 1.1886, R2 Val : 0.9143  Time for epoch 441: 1.67 seconds\n",
      "Epoch [442/500], Train Loss: 0.0385, Train R2 Loss: 0.0698, Val Loss: 1.1746, R2 Val : 0.9302  Time for epoch 442: 1.67 seconds\n",
      "Epoch [443/500], Train Loss: 0.0673, Train R2 Loss: 0.0073, Val Loss: 1.3212, R2 Val : 0.9927  Time for epoch 443: 1.69 seconds\n",
      "Epoch [444/500], Train Loss: 0.1838, Train R2 Loss: 0.0805, Val Loss: 1.3665, R2 Val : 0.9195  Time for epoch 444: 1.77 seconds\n",
      "Epoch [445/500], Train Loss: 0.0055, Train R2 Loss: 0.0267, Val Loss: 1.1205, R2 Val : 0.9733  Time for epoch 445: 1.69 seconds\n",
      "Epoch [446/500], Train Loss: 0.0665, Train R2 Loss: 0.0406, Val Loss: 1.1121, R2 Val : 0.9594  Time for epoch 446: 1.68 seconds\n",
      "Epoch [447/500], Train Loss: 0.0321, Train R2 Loss: 0.0500, Val Loss: 1.0019, R2 Val : 0.9500  Time for epoch 447: 1.68 seconds\n",
      "Epoch [448/500], Train Loss: 0.8345, Train R2 Loss: 0.8238, Val Loss: 1.4565, R2 Val : 0.1762  Time for epoch 448: 1.69 seconds\n",
      "Epoch [449/500], Train Loss: 0.0483, Train R2 Loss: 0.0782, Val Loss: 1.0266, R2 Val : 0.9218  Time for epoch 449: 1.68 seconds\n",
      "Epoch [450/500], Train Loss: 0.0701, Train R2 Loss: 0.0502, Val Loss: 0.9726, R2 Val : 0.9498  Time for epoch 450: 1.69 seconds\n",
      "Epoch [451/500], Train Loss: 0.1422, Train R2 Loss: 0.0725, Val Loss: 1.3247, R2 Val : 0.9275  Time for epoch 451: 1.70 seconds\n",
      "Epoch [452/500], Train Loss: 0.1392, Train R2 Loss: 0.0437, Val Loss: 0.9638, R2 Val : 0.9563  Time for epoch 452: 1.69 seconds\n",
      "Epoch [453/500], Train Loss: 0.0431, Train R2 Loss: 0.0257, Val Loss: 1.0314, R2 Val : 0.9743  Time for epoch 453: 1.70 seconds\n",
      "Epoch [454/500], Train Loss: 0.0471, Train R2 Loss: 0.0515, Val Loss: 0.9771, R2 Val : 0.9485  Time for epoch 454: 1.68 seconds\n",
      "Epoch [455/500], Train Loss: 0.0768, Train R2 Loss: 0.0479, Val Loss: 1.3628, R2 Val : 0.9521  Time for epoch 455: 1.68 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [456/500], Train Loss: 0.1247, Train R2 Loss: 0.0342, Val Loss: 0.9970, R2 Val : 0.9658  Time for epoch 456: 1.70 seconds\n",
      "Epoch [457/500], Train Loss: 0.2055, Train R2 Loss: 0.0687, Val Loss: 0.9159, R2 Val : 0.9313  Time for epoch 457: 1.68 seconds\n",
      "Epoch [458/500], Train Loss: 0.1349, Train R2 Loss: 0.0239, Val Loss: 0.7158, R2 Val : 0.9761  Time for epoch 458: 1.69 seconds\n",
      "Epoch [459/500], Train Loss: 0.0929, Train R2 Loss: 0.0313, Val Loss: 0.9039, R2 Val : 0.9687  Time for epoch 459: 1.69 seconds\n",
      "Epoch [460/500], Train Loss: 0.0074, Train R2 Loss: 0.0256, Val Loss: 0.7286, R2 Val : 0.9744  Time for epoch 460: 1.69 seconds\n",
      "Epoch [461/500], Train Loss: 0.0897, Train R2 Loss: 0.0427, Val Loss: 0.9175, R2 Val : 0.9573  Time for epoch 461: 1.68 seconds\n",
      "Epoch [462/500], Train Loss: 0.0573, Train R2 Loss: 0.0282, Val Loss: 0.9204, R2 Val : 0.9718  Time for epoch 462: 1.68 seconds\n",
      "Epoch [463/500], Train Loss: 0.0440, Train R2 Loss: 0.0353, Val Loss: 0.8774, R2 Val : 0.9647  Time for epoch 463: 1.71 seconds\n",
      "Epoch [464/500], Train Loss: 0.0162, Train R2 Loss: 0.0345, Val Loss: 0.7708, R2 Val : 0.9655  Time for epoch 464: 1.67 seconds\n",
      "Epoch [465/500], Train Loss: 0.2314, Train R2 Loss: 0.1730, Val Loss: 1.2115, R2 Val : 0.8270  Time for epoch 465: 1.67 seconds\n",
      "Epoch [466/500], Train Loss: 0.0109, Train R2 Loss: 0.0284, Val Loss: 0.8080, R2 Val : 0.9716  Time for epoch 466: 1.68 seconds\n",
      "Epoch [467/500], Train Loss: 0.0528, Train R2 Loss: 0.0166, Val Loss: 0.9751, R2 Val : 0.9834  Time for epoch 467: 1.67 seconds\n",
      "Epoch [468/500], Train Loss: 0.0682, Train R2 Loss: 0.0560, Val Loss: 0.8669, R2 Val : 0.9440  Time for epoch 468: 1.67 seconds\n",
      "Epoch [469/500], Train Loss: 0.0197, Train R2 Loss: 0.0510, Val Loss: 1.1962, R2 Val : 0.9490  Time for epoch 469: 1.69 seconds\n",
      "Epoch [470/500], Train Loss: 0.0719, Train R2 Loss: 0.1256, Val Loss: 1.4096, R2 Val : 0.8744  Time for epoch 470: 1.67 seconds\n",
      "Epoch [471/500], Train Loss: 0.0102, Train R2 Loss: 0.0113, Val Loss: 1.0496, R2 Val : 0.9887  Time for epoch 471: 1.76 seconds\n",
      "Epoch [472/500], Train Loss: 0.1228, Train R2 Loss: 0.0440, Val Loss: 1.2081, R2 Val : 0.9560  Time for epoch 472: 1.75 seconds\n",
      "Epoch [473/500], Train Loss: 0.1198, Train R2 Loss: 0.0308, Val Loss: 1.1153, R2 Val : 0.9692  Time for epoch 473: 1.77 seconds\n",
      "Epoch [474/500], Train Loss: 0.1853, Train R2 Loss: 0.2047, Val Loss: 1.4551, R2 Val : 0.7953  Time for epoch 474: 1.69 seconds\n",
      "Epoch [475/500], Train Loss: 0.0128, Train R2 Loss: 0.0510, Val Loss: 0.8834, R2 Val : 0.9490  Time for epoch 475: 1.73 seconds\n",
      "Epoch [476/500], Train Loss: 0.0046, Train R2 Loss: 0.0713, Val Loss: 1.4676, R2 Val : 0.9287  Time for epoch 476: 1.74 seconds\n",
      "Epoch [477/500], Train Loss: 0.0933, Train R2 Loss: 0.0550, Val Loss: 1.4190, R2 Val : 0.9450  Time for epoch 477: 1.69 seconds\n",
      "Epoch [478/500], Train Loss: 0.1069, Train R2 Loss: 0.1086, Val Loss: 1.2680, R2 Val : 0.8914  Time for epoch 478: 1.68 seconds\n",
      "Epoch [479/500], Train Loss: 0.0023, Train R2 Loss: 0.0434, Val Loss: 1.1091, R2 Val : 0.9566  Time for epoch 479: 1.68 seconds\n",
      "Epoch [480/500], Train Loss: 0.0033, Train R2 Loss: 0.0781, Val Loss: 1.2051, R2 Val : 0.9219  Time for epoch 480: 1.68 seconds\n",
      "Epoch [481/500], Train Loss: 0.0568, Train R2 Loss: 0.0387, Val Loss: 0.9097, R2 Val : 0.9613  Time for epoch 481: 1.69 seconds\n",
      "Epoch [482/500], Train Loss: 0.0934, Train R2 Loss: 0.0411, Val Loss: 1.1755, R2 Val : 0.9589  Time for epoch 482: 1.70 seconds\n",
      "Epoch [483/500], Train Loss: 0.0074, Train R2 Loss: 0.0503, Val Loss: 1.0270, R2 Val : 0.9497  Time for epoch 483: 1.70 seconds\n",
      "Epoch [484/500], Train Loss: 0.0923, Train R2 Loss: 0.0535, Val Loss: 1.0247, R2 Val : 0.9465  Time for epoch 484: 1.69 seconds\n",
      "Epoch [485/500], Train Loss: 0.0807, Train R2 Loss: 0.0232, Val Loss: 0.9762, R2 Val : 0.9768  Time for epoch 485: 1.72 seconds\n",
      "Epoch [486/500], Train Loss: 0.0958, Train R2 Loss: 0.0218, Val Loss: 0.7700, R2 Val : 0.9782  Time for epoch 486: 1.69 seconds\n",
      "Epoch [487/500], Train Loss: 0.1635, Train R2 Loss: 0.0776, Val Loss: 0.8599, R2 Val : 0.9224  Time for epoch 487: 1.68 seconds\n",
      "Epoch [488/500], Train Loss: 0.0102, Train R2 Loss: 0.0786, Val Loss: 1.1634, R2 Val : 0.9214  Time for epoch 488: 1.69 seconds\n",
      "Epoch [489/500], Train Loss: 0.0046, Train R2 Loss: 0.0304, Val Loss: 0.9770, R2 Val : 0.9696  Time for epoch 489: 1.67 seconds\n",
      "Epoch [490/500], Train Loss: 0.0087, Train R2 Loss: 0.0204, Val Loss: 0.8023, R2 Val : 0.9796  Time for epoch 490: 1.67 seconds\n",
      "Epoch [491/500], Train Loss: 0.1239, Train R2 Loss: 0.0379, Val Loss: 1.1044, R2 Val : 0.9621  Time for epoch 491: 1.68 seconds\n",
      "Epoch [492/500], Train Loss: 0.2577, Train R2 Loss: 0.0999, Val Loss: 0.6911, R2 Val : 0.9001  Time for epoch 492: 1.68 seconds\n",
      "Epoch [493/500], Train Loss: 0.8482, Train R2 Loss: 0.0651, Val Loss: 0.8906, R2 Val : 0.9349  Time for epoch 493: 1.67 seconds\n",
      "Epoch [494/500], Train Loss: 0.3211, Train R2 Loss: 0.1529, Val Loss: 1.0500, R2 Val : 0.8471  Time for epoch 494: 1.68 seconds\n",
      "Epoch [495/500], Train Loss: 0.1430, Train R2 Loss: 0.0416, Val Loss: 0.9653, R2 Val : 0.9584  Time for epoch 495: 1.67 seconds\n",
      "Epoch [496/500], Train Loss: 0.2817, Train R2 Loss: 0.1910, Val Loss: 1.2432, R2 Val : 0.8090  Time for epoch 496: 1.68 seconds\n",
      "Epoch [497/500], Train Loss: 0.1607, Train R2 Loss: 0.1188, Val Loss: 1.1903, R2 Val : 0.8812  Time for epoch 497: 1.68 seconds\n",
      "Epoch [498/500], Train Loss: 0.0790, Train R2 Loss: 0.0223, Val Loss: 0.8328, R2 Val : 0.9777  Time for epoch 498: 1.67 seconds\n",
      "Epoch [499/500], Train Loss: 0.0341, Train R2 Loss: 0.0538, Val Loss: 1.0327, R2 Val : 0.9462  Time for epoch 499: 1.68 seconds\n",
      "Epoch [500/500], Train Loss: 0.0343, Train R2 Loss: 0.0173, Val Loss: 0.8610, R2 Val : 0.9827  Time for epoch 500: 1.67 seconds\n",
      "Total time 849.1347162723541\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Loop over the epochs\n",
    "num_epochs = 500\n",
    "\n",
    "begin_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    # Training\n",
    "    model.train()\n",
    "    for inputs, labels in train_dataloader:\n",
    "        tr_pred =[]\n",
    "        tr_true =[]\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.squeeze(0).squeeze(0).to(device)\n",
    "        labels = labels.to(device)\n",
    "        x_categ = torch.Tensor([[int(inputs[0]),int(inputs[1])]]).long().to(device)\n",
    "        x_numec = torch.Tensor([[float(inputs[2]),float(inputs[3]),float(inputs[4]),float(inputs[5]),float(inputs[6]),float(inputs[7]),float(inputs[8]),float(inputs[9]),float(inputs[10])]]).to(device)\n",
    "        outputs = model(x_categ,x_numec)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tr_pred.append(outputs[0][0].data.detach().cpu().numpy())\n",
    "        tr_true.append(labels[0].data.detach().cpu().numpy())\n",
    "    \n",
    "            \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        val_loss_r2 = 0.0\n",
    "        tst_pred =[]\n",
    "        tst_true =[]\n",
    "        for inputs, labels in val_dataloader:\n",
    "            inputs = inputs.squeeze(0).squeeze(0).to(device)\n",
    "            labels = labels.to(device)\n",
    "            x_categ_val = torch.Tensor([[int(inputs[0]),int(inputs[1])]]).long().to(device)\n",
    "            x_numec_val = torch.Tensor([[float(inputs[2]),float(inputs[3]),float(inputs[4]),float(inputs[5]),float(inputs[6]),float(inputs[7]),float(inputs[8]),float(inputs[9]),float(inputs[10])]]).to(device)\n",
    "            outputs = model(x_categ_val, x_numec_val)\n",
    "            val_loss += criterion(outputs, labels).item() * inputs.shape[0]\n",
    "#             val_loss_r2 += r2_torch_score(outputs[0][0], labels[0]).item() * inputs.shape[0]\n",
    "            tr_pred.append(outputs[0][0].data.detach().cpu().numpy())\n",
    "            tr_true.append(labels[0].data.detach().cpu().numpy())\n",
    "        val_loss /= len(val_dataset)\n",
    "        val_loss_r2 /= len(val_dataset)\n",
    "        \n",
    "    loss_r2 =( 1- r2_score(tr_true, tr_pred))\n",
    "    score_r2 = r2_score(tr_true, tr_pred)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    \n",
    "    # Save the model after each epoch\n",
    "    model_file = './weight/'+case_no+'/'+ f\"model_epoch_{epoch + 1}_{score_r2:.4f}.pth\"\n",
    "    torch.save(model, model_file)\n",
    "    \n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {loss.item():.4f}, Train R2 Loss: {loss_r2:.4f}, Val Loss: {val_loss:.4f}, R2 Val : {score_r2:.4f}\",f\" Time for epoch {epoch + 1}: {epoch_time:.2f} seconds\")\n",
    "finish_time = time.time()    \n",
    "tot_time = finish_time - begin_time \n",
    "print(f\"Total time {tot_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 11)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.69155\n",
      "140.02486\n",
      "95.362915\n",
      "169.54025\n",
      "192.97713\n",
      "98.15507\n",
      "154.1566\n",
      "184.76297\n",
      "173.84525\n",
      "156.75713\n",
      "48.594982\n",
      "35.9335\n",
      "39.826572\n",
      "61.444508\n",
      "108.296\n",
      "43.359364\n",
      "63.850708\n",
      "103.053\n",
      "82.808784\n",
      "39.783978\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "model.eval()\n",
    "prediction = []\n",
    "for i in range(X_test.shape[0]):\n",
    "    x_categ_val = torch.Tensor([[X_test[i][0][0],X_test[i][0][1]]]).long().to(device)\n",
    "    x_num_val = torch.Tensor([[X_test[i][0][2],X_test[i][0][3],X_test[i][0][4],X_test[i][0][5],X_test[i][0][6],X_test[i][0][7],X_test[i][0][8],X_test[i][0][9],X_test[i][0][10]]]).to(device)\n",
    "#     inputs = torch.Tensor(X_test[i]).to(device)\n",
    "    y_pred = model(x_categ_val,x_num_val)\n",
    "    y_pred = scaler.inverse_transform(y_pred[0][0].data.detach().cpu().numpy().reshape(-1, 1))\n",
    "    print(y_pred[0][0])\n",
    "\n",
    "    prediction.append(y_pred[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m Mean Squared Error:  217.34428\n",
      "\u001b[91m R-squared:  0.9175391711787944\n"
     ]
    }
   ],
   "source": [
    "if len(y_pred.shape)>=3:\n",
    "    y_pred = tf.squeeze(y_pred, axis=-1)\n",
    "    y_pred = tf.squeeze(y_pred, axis=-1)\n",
    "\n",
    "# Compare the predictions with the ground truth\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print('\\033[91m Mean Squared Error: ', mse)\n",
    "print('\\033[91m R-squared: ', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119.0 :==: 90.69155\n",
      "114.0 :==: 140.02486\n",
      "110.0 :==: 95.362915\n",
      "177.0 :==: 169.54025\n",
      "212.0 :==: 192.97713\n",
      "97.0 :==: 98.15507\n",
      "126.0 :==: 154.1566\n",
      "188.0 :==: 184.76297\n",
      "188.0 :==: 173.84525\n",
      "124.0 :==: 156.75713\n",
      "52.0 :==: 48.594982\n",
      "40.0 :==: 35.9335\n",
      "43.0 :==: 39.826572\n",
      "67.0 :==: 61.444508\n",
      "110.0 :==: 108.296\n",
      "46.0 :==: 43.359364\n",
      "66.0 :==: 63.850708\n",
      "100.0 :==: 103.053\n",
      "88.0 :==: 82.808784\n",
      "46.0 :==: 39.783978\n",
      "mean absolute error: 10.60341\n",
      "absolute_mean_percentage_error: 9.885715\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i,j in zip(y_test, y_pred):\n",
    "    try:\n",
    "        print(i,\":==:\",j)\n",
    "    except:\n",
    "        print(i,\":==:\",j.numpy())   \n",
    "    \n",
    "errors = np.abs(np.array(y_test) - np.array(y_pred))\n",
    "mean_absolute_error = np.mean(errors)\n",
    "mape = mape_loss(y_test, y_pred).numpy()\n",
    "\n",
    "print(\"mean absolute error:\", mean_absolute_error)\n",
    "print(\"absolute_mean_percentage_error:\", mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = natsorted(glob.glob('./weight/'+case_no+'/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./weight/045\\\\model_epoch_217_0.6175.pth'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weights[240]\n",
    "weights[216]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best = model = torch.load(weights[216]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.986534\n",
      "119.88882\n",
      "99.927826\n",
      "175.2175\n",
      "204.8465\n",
      "100.03241\n",
      "138.45464\n",
      "191.19417\n",
      "184.19427\n",
      "148.2673\n",
      "54.739704\n",
      "43.36855\n",
      "50.66457\n",
      "65.73337\n",
      "113.441414\n",
      "51.24905\n",
      "59.24619\n",
      "104.34665\n",
      "92.07425\n",
      "46.013683\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "model_best.eval()\n",
    "prediction = []\n",
    "for i in range(X_test.shape[0]):\n",
    "    x_categ_val = torch.Tensor([[X_test[i][0][0],X_test[i][0][1]]]).long().to(device)\n",
    "    x_num_val = torch.Tensor([[X_test[i][0][2],X_test[i][0][3],X_test[i][0][4],X_test[i][0][5],X_test[i][0][6],X_test[i][0][7],X_test[i][0][8],X_test[i][0][9],X_test[i][0][10]]]).to(device)\n",
    "#     inputs = torch.Tensor(X_test[i]).to(device)\n",
    "    y_pred = model(x_categ_val,x_num_val)\n",
    "#     y_pred = model_best(x_categ_val,inputs)\n",
    "    y_pred = scaler.inverse_transform(y_pred[0][0].data.detach().cpu().numpy().reshape(-1, 1))\n",
    "    print(y_pred[0][0])\n",
    "\n",
    "    prediction.append(y_pred[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m Mean Squared Error:  78.43117\n",
      "\u001b[91m R-squared:  0.9702430680383609\n"
     ]
    }
   ],
   "source": [
    "if len(y_pred.shape)>=3:\n",
    "    y_pred = tf.squeeze(y_pred, axis=-1)\n",
    "    y_pred = tf.squeeze(y_pred, axis=-1)\n",
    "# Compare the predictions with the ground truth\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "# r2 = np.corrcoef(y_pred, y_test)[0, 1] ** 2\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print('\\033[91m Mean Squared Error: ', mse)\n",
    "print('\\033[91m R-squared: ', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119.0 :==: 98.986534\n",
      "114.0 :==: 119.88882\n",
      "110.0 :==: 99.927826\n",
      "177.0 :==: 175.2175\n",
      "212.0 :==: 204.8465\n",
      "97.0 :==: 100.03241\n",
      "126.0 :==: 138.45464\n",
      "188.0 :==: 191.19417\n",
      "188.0 :==: 184.19427\n",
      "124.0 :==: 148.2673\n",
      "52.0 :==: 54.739704\n",
      "40.0 :==: 43.36855\n",
      "43.0 :==: 50.66457\n",
      "67.0 :==: 65.73337\n",
      "110.0 :==: 113.441414\n",
      "46.0 :==: 51.24905\n",
      "66.0 :==: 59.24619\n",
      "100.0 :==: 104.34665\n",
      "88.0 :==: 92.07425\n",
      "46.0 :==: 46.013683\n",
      "mean absolute error: 6.529151\n",
      "absolute_percentage_errors: 6.950502\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(y_test, y_pred):\n",
    "    try:\n",
    "        print(i,\":==:\",j)\n",
    "    except:\n",
    "        print(i,\":==:\",j.numpy())\n",
    "errors = np.abs(np.array(y_test) - np.array(y_pred))\n",
    "mean_absolute_error = np.mean(errors)\n",
    "mape = mape_loss(y_test, y_pred).numpy()\n",
    "# mape = tf.keras.losses.MAPE(y_test, y_pred).numpy()\n",
    "\n",
    "\n",
    "print(\"mean absolute error:\", mean_absolute_error)\n",
    "# print(\"absolute_mean_percentage_error:\", absolute_mean_percentage_error)\n",
    "print(\"absolute_percentage_errors:\", mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1  :  0.5258\n",
      "model 2  :  0.7313\n",
      "model 3  :  0.5839\n",
      "model 4  :  0.6474\n",
      "model 5  :  0.7641\n",
      "model 6  :  0.8255\n",
      "model 7  :  0.7496\n",
      "model 8  :  0.8667\n",
      "model 9  :  0.7336\n",
      "model 10  :  0.7732\n",
      "model 11  :  0.7393\n",
      "model 12  :  0.7625\n",
      "model 13  :  0.5374\n",
      "model 14  :  0.7698\n",
      "model 15  :  0.8012\n",
      "model 16  :  0.8422\n",
      "model 17  :  0.8043\n",
      "model 18  :  0.7722\n",
      "model 19  :  0.8885\n",
      "model 20  :  0.8472\n",
      "model 21  :  0.7971\n",
      "model 22  :  0.5677\n",
      "model 23  :  0.9067\n",
      "model 24  :  0.8688\n",
      "model 25  :  0.8312\n",
      "model 26  :  0.8909\n",
      "model 27  :  0.6968\n",
      "model 28  :  0.8332\n",
      "model 29  :  0.8598\n",
      "model 30  :  0.9057\n",
      "model 31  :  0.8545\n",
      "model 32  :  0.91\n",
      "model 33  :  0.9634\n",
      "model 34  :  0.9115\n",
      "model 35  :  0.9389\n",
      "model 36  :  0.7164\n",
      "model 37  :  0.8631\n",
      "model 38  :  0.8219\n",
      "model 39  :  0.8917\n",
      "model 40  :  0.9108\n",
      "model 41  :  0.9109\n",
      "model 42  :  0.9303\n",
      "model 43  :  0.8894\n",
      "model 44  :  0.9406\n",
      "model 45  :  0.8218\n",
      "model 46  :  0.9303\n",
      "model 47  :  0.8146\n",
      "model 48  :  0.9324\n",
      "model 49  :  0.9215\n",
      "model 50  :  0.923\n",
      "model 51  :  0.91\n",
      "model 52  :  0.9472\n",
      "model 53  :  0.9387\n",
      "model 54  :  0.9411\n",
      "model 55  :  0.9343\n",
      "model 56  :  0.855\n",
      "model 57  :  0.922\n",
      "model 58  :  0.8794\n",
      "model 59  :  0.8984\n",
      "model 60  :  0.9003\n",
      "model 61  :  0.9258\n",
      "model 62  :  0.9003\n",
      "model 63  :  0.9184\n",
      "model 64  :  0.8403\n",
      "model 65  :  0.8902\n",
      "model 66  :  0.9065\n",
      "model 67  :  0.902\n",
      "model 68  :  0.7377\n",
      "model 69  :  0.9093\n",
      "model 70  :  0.8509\n",
      "model 71  :  0.9155\n",
      "model 72  :  0.951\n",
      "model 73  :  0.9059\n",
      "model 74  :  0.8926\n",
      "model 75  :  0.8257\n",
      "model 76  :  0.7832\n",
      "model 77  :  0.926\n",
      "model 78  :  0.9358\n",
      "model 79  :  0.9149\n",
      "model 80  :  0.8696\n",
      "model 81  :  0.9422\n",
      "model 82  :  0.9101\n",
      "model 83  :  0.9276\n",
      "model 84  :  0.85\n",
      "model 85  :  0.8773\n",
      "model 86  :  0.9354\n",
      "model 87  :  0.9212\n",
      "model 88  :  0.9215\n",
      "model 89  :  0.9341\n",
      "model 90  :  0.9116\n",
      "model 91  :  0.9259\n",
      "model 92  :  0.904\n",
      "model 93  :  0.9256\n",
      "model 94  :  0.9253\n",
      "model 95  :  0.9238\n",
      "model 96  :  0.9302\n",
      "model 97  :  0.9321\n",
      "model 98  :  0.9282\n",
      "model 99  :  0.923\n",
      "model 100  :  0.9462\n",
      "model 101  :  0.9399\n",
      "model 102  :  0.9285\n",
      "model 103  :  0.9415\n",
      "model 104  :  0.9428\n",
      "model 105  :  0.9315\n",
      "model 106  :  0.8497\n",
      "model 107  :  0.9429\n",
      "model 108  :  0.9192\n",
      "model 109  :  0.9455\n",
      "model 110  :  0.9204\n",
      "model 111  :  0.911\n",
      "model 112  :  0.8852\n",
      "model 113  :  0.936\n",
      "model 114  :  0.9392\n",
      "model 115  :  0.9497\n",
      "model 116  :  0.9168\n",
      "model 117  :  0.9317\n",
      "model 118  :  0.9337\n",
      "model 119  :  0.931\n",
      "model 120  :  0.9316\n",
      "model 121  :  0.8953\n",
      "model 122  :  0.9252\n",
      "model 123  :  0.9323\n",
      "model 124  :  0.932\n",
      "model 125  :  0.9411\n",
      "model 126  :  0.8989\n",
      "model 127  :  0.93\n",
      "model 128  :  0.9321\n",
      "model 129  :  0.9457\n",
      "model 130  :  0.924\n",
      "model 131  :  0.9228\n",
      "model 132  :  0.9127\n",
      "model 133  :  0.9266\n",
      "model 134  :  0.9091\n",
      "model 135  :  0.9289\n",
      "model 136  :  0.905\n",
      "model 137  :  0.9377\n",
      "model 138  :  0.9367\n",
      "model 139  :  0.9173\n",
      "model 140  :  0.9284\n",
      "model 141  :  0.9426\n",
      "model 142  :  0.8821\n",
      "model 143  :  0.959\n",
      "model 144  :  0.9472\n",
      "model 145  :  0.915\n",
      "model 146  :  0.9312\n",
      "model 147  :  0.9567\n",
      "model 148  :  0.953\n",
      "model 149  :  0.9476\n",
      "model 150  :  0.928\n",
      "model 151  :  0.9338\n",
      "model 152  :  0.9469\n",
      "model 153  :  0.9463\n",
      "model 154  :  0.9448\n",
      "model 155  :  0.939\n",
      "model 156  :  0.9314\n",
      "model 157  :  0.9451\n",
      "model 158  :  0.9469\n",
      "model 159  :  0.9553\n",
      "model 160  :  0.9559\n",
      "model 161  :  0.9524\n",
      "model 162  :  0.952\n",
      "model 163  :  0.9377\n",
      "model 164  :  0.9491\n",
      "model 165  :  0.9436\n",
      "model 166  :  0.9629\n",
      "model 167  :  0.9394\n",
      "model 168  :  0.9317\n",
      "model 169  :  0.9244\n",
      "model 170  :  0.9454\n",
      "model 171  :  0.9463\n",
      "model 172  :  0.9313\n",
      "model 173  :  0.9323\n",
      "model 174  :  0.9164\n",
      "model 175  :  0.9431\n",
      "model 176  :  0.9455\n",
      "model 177  :  0.9433\n",
      "model 178  :  0.9361\n",
      "model 179  :  0.9525\n",
      "model 180  :  0.9447\n",
      "model 181  :  0.9505\n",
      "model 182  :  0.9467\n",
      "model 183  :  0.9296\n",
      "model 184  :  0.9167\n",
      "model 185  :  0.9363\n",
      "model 186  :  0.9445\n",
      "model 187  :  0.928\n",
      "model 188  :  0.9475\n",
      "model 189  :  0.9508\n",
      "model 190  :  0.937\n",
      "model 191  :  0.9049\n",
      "model 192  :  0.9398\n",
      "model 193  :  0.9516\n",
      "model 194  :  0.9596\n",
      "model 195  :  0.942\n",
      "model 196  :  0.9553\n",
      "model 197  :  0.9662\n",
      "model 198  :  0.9453\n",
      "model 199  :  0.9534\n",
      "model 200  :  0.9381\n",
      "model 201  :  0.9459\n",
      "model 202  :  0.9553\n",
      "model 203  :  0.9479\n",
      "model 204  :  0.9533\n",
      "model 205  :  0.958\n",
      "model 206  :  0.9597\n",
      "model 207  :  0.9498\n",
      "model 208  :  0.9413\n",
      "model 209  :  0.9336\n",
      "model 210  :  0.9494\n",
      "model 211  :  0.9214\n",
      "model 212  :  0.8919\n",
      "model 213  :  0.9004\n",
      "model 214  :  0.9205\n",
      "model 215  :  0.9116\n",
      "model 216  :  0.9389\n",
      "model 217  :  0.9702\n",
      "model 218  :  0.9354\n",
      "model 219  :  0.9514\n",
      "model 220  :  0.9626\n",
      "model 221  :  0.9517\n",
      "model 222  :  0.9561\n",
      "model 223  :  0.9483\n",
      "model 224  :  0.9588\n",
      "model 225  :  0.9596\n",
      "model 226  :  0.9316\n",
      "model 227  :  0.9082\n",
      "model 228  :  0.9409\n",
      "model 229  :  0.9544\n",
      "model 230  :  0.9548\n",
      "model 231  :  0.947\n",
      "model 232  :  0.9425\n",
      "model 233  :  0.9574\n",
      "model 234  :  0.9576\n",
      "model 235  :  0.9439\n",
      "model 236  :  0.9519\n",
      "model 237  :  0.9344\n",
      "model 238  :  0.9552\n",
      "model 239  :  0.9583\n",
      "model 240  :  0.9556\n",
      "model 241  :  0.9648\n",
      "model 242  :  0.9402\n",
      "model 243  :  0.9434\n",
      "model 244  :  0.957\n",
      "model 245  :  0.9163\n",
      "model 246  :  0.9254\n",
      "model 247  :  0.9539\n",
      "model 248  :  0.9504\n",
      "model 249  :  0.9332\n",
      "model 250  :  0.9507\n",
      "model 251  :  0.9157\n",
      "model 252  :  0.9454\n",
      "model 253  :  0.9342\n",
      "model 254  :  0.9351\n",
      "model 255  :  0.9507\n",
      "model 256  :  0.9533\n",
      "model 257  :  0.9436\n",
      "model 258  :  0.9367\n",
      "model 259  :  0.9302\n",
      "model 260  :  0.8928\n",
      "model 261  :  0.9252\n",
      "model 262  :  0.9339\n",
      "model 263  :  0.9275\n",
      "model 264  :  0.9325\n",
      "model 265  :  0.9382\n",
      "model 266  :  0.937\n",
      "model 267  :  0.9057\n",
      "model 268  :  0.9324\n",
      "model 269  :  0.9399\n",
      "model 270  :  0.9394\n",
      "model 271  :  0.9597\n",
      "model 272  :  0.9488\n",
      "model 273  :  0.9293\n",
      "model 274  :  0.9188\n",
      "model 275  :  0.9513\n",
      "model 276  :  0.9283\n",
      "model 277  :  0.9283\n",
      "model 278  :  0.9162\n",
      "model 279  :  0.9378\n",
      "model 280  :  0.9578\n",
      "model 281  :  0.9451\n",
      "model 282  :  0.9432\n",
      "model 283  :  0.9268\n",
      "model 284  :  0.9475\n",
      "model 285  :  0.9474\n",
      "model 286  :  0.9352\n",
      "model 287  :  0.9263\n",
      "model 288  :  0.9595\n",
      "model 289  :  0.9265\n",
      "model 290  :  0.933\n",
      "model 291  :  0.9469\n",
      "model 292  :  0.9497\n",
      "model 293  :  0.9278\n",
      "model 294  :  0.9403\n",
      "model 295  :  0.9459\n",
      "model 296  :  0.9233\n",
      "model 297  :  0.9403\n",
      "model 298  :  0.9233\n",
      "model 299  :  0.9579\n",
      "model 300  :  0.936\n",
      "model 301  :  0.9468\n",
      "model 302  :  0.9428\n",
      "model 303  :  0.9281\n",
      "model 304  :  0.9422\n",
      "model 305  :  0.9495\n",
      "model 306  :  0.9272\n",
      "model 307  :  0.9554\n",
      "model 308  :  0.956\n",
      "model 309  :  0.9501\n",
      "model 310  :  0.9576\n",
      "model 311  :  0.9604\n",
      "model 312  :  0.9501\n",
      "model 313  :  0.9557\n",
      "model 314  :  0.9315\n",
      "model 315  :  0.9469\n",
      "model 316  :  0.9278\n",
      "model 317  :  0.9406\n",
      "model 318  :  0.9387\n",
      "model 319  :  0.9559\n",
      "model 320  :  0.9456\n",
      "model 321  :  0.9417\n",
      "model 322  :  0.9555\n",
      "model 323  :  0.9519\n",
      "model 324  :  0.938\n",
      "model 325  :  0.9389\n",
      "model 326  :  0.9582\n",
      "model 327  :  0.9502\n",
      "model 328  :  0.9507\n",
      "model 329  :  0.9559\n",
      "model 330  :  0.9462\n",
      "model 331  :  0.948\n",
      "model 332  :  0.953\n",
      "model 333  :  0.9285\n",
      "model 334  :  0.9539\n",
      "model 335  :  0.9536\n",
      "model 336  :  0.9305\n",
      "model 337  :  0.9184\n",
      "model 338  :  0.9364\n",
      "model 339  :  0.9271\n",
      "model 340  :  0.949\n",
      "model 341  :  0.9537\n",
      "model 342  :  0.9289\n",
      "model 343  :  0.9316\n",
      "model 344  :  0.9385\n",
      "model 345  :  0.9446\n",
      "model 346  :  0.9289\n",
      "model 347  :  0.9526\n",
      "model 348  :  0.9525\n",
      "model 349  :  0.9517\n",
      "model 350  :  0.9646\n",
      "model 351  :  0.9298\n",
      "model 352  :  0.9548\n",
      "model 353  :  0.8968\n",
      "model 354  :  0.9445\n",
      "model 355  :  0.9487\n",
      "model 356  :  0.9174\n",
      "model 357  :  0.9328\n",
      "model 358  :  0.9447\n",
      "model 359  :  0.9391\n",
      "model 360  :  0.9568\n",
      "model 361  :  0.9369\n",
      "model 362  :  0.9427\n",
      "model 363  :  0.9432\n",
      "model 364  :  0.9261\n",
      "model 365  :  0.9528\n",
      "model 366  :  0.9346\n",
      "model 367  :  0.9599\n",
      "model 368  :  0.943\n",
      "model 369  :  0.956\n",
      "model 370  :  0.9472\n",
      "model 371  :  0.9248\n",
      "model 372  :  0.9477\n",
      "model 373  :  0.8634\n",
      "model 374  :  0.9381\n",
      "model 375  :  0.9423\n",
      "model 376  :  0.9282\n",
      "model 377  :  0.9408\n",
      "model 378  :  0.9205\n",
      "model 379  :  0.9238\n",
      "model 380  :  0.9277\n",
      "model 381  :  0.9415\n",
      "model 382  :  0.9108\n",
      "model 383  :  0.9141\n",
      "model 384  :  0.9504\n",
      "model 385  :  0.9308\n",
      "model 386  :  0.9498\n",
      "model 387  :  0.9229\n",
      "model 388  :  0.9252\n",
      "model 389  :  0.9361\n",
      "model 390  :  0.931\n",
      "model 391  :  0.9359\n",
      "model 392  :  0.9425\n",
      "model 393  :  0.929\n",
      "model 394  :  0.9257\n",
      "model 395  :  0.9429\n",
      "model 396  :  0.8897\n",
      "model 397  :  0.9328\n",
      "model 398  :  0.9298\n",
      "model 399  :  0.9499\n",
      "model 400  :  0.9468\n",
      "model 401  :  0.916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 402  :  0.93\n",
      "model 403  :  0.924\n",
      "model 404  :  0.9407\n",
      "model 405  :  0.9311\n",
      "model 406  :  0.8068\n",
      "model 407  :  0.8635\n",
      "model 408  :  0.9391\n",
      "model 409  :  0.9241\n",
      "model 410  :  0.8969\n",
      "model 411  :  0.9092\n",
      "model 412  :  0.9378\n",
      "model 413  :  0.8926\n",
      "model 414  :  0.953\n",
      "model 415  :  0.9249\n",
      "model 416  :  0.9374\n",
      "model 417  :  0.9229\n",
      "model 418  :  0.9125\n",
      "model 419  :  0.9282\n",
      "model 420  :  0.9267\n",
      "model 421  :  0.9324\n",
      "model 422  :  0.9\n",
      "model 423  :  0.8964\n",
      "model 424  :  0.929\n",
      "model 425  :  0.9205\n",
      "model 426  :  0.9106\n",
      "model 427  :  0.8629\n",
      "model 428  :  0.9155\n",
      "model 429  :  0.8575\n",
      "model 430  :  0.8887\n",
      "model 431  :  0.9131\n",
      "model 432  :  0.8823\n",
      "model 433  :  0.8622\n",
      "model 434  :  0.9164\n",
      "model 435  :  0.9176\n",
      "model 436  :  0.9234\n",
      "model 437  :  0.9102\n",
      "model 438  :  0.9096\n",
      "model 439  :  0.8966\n",
      "model 440  :  0.9172\n",
      "model 441  :  0.9136\n",
      "model 442  :  0.907\n",
      "model 443  :  0.9056\n",
      "model 444  :  0.8604\n",
      "model 445  :  0.9286\n",
      "model 446  :  0.8917\n",
      "model 447  :  0.9088\n",
      "model 448  :  0.9016\n",
      "model 449  :  0.9269\n",
      "model 450  :  0.9217\n",
      "model 451  :  0.8716\n",
      "model 452  :  0.8852\n",
      "model 453  :  0.9074\n",
      "model 454  :  0.9163\n",
      "model 455  :  0.927\n",
      "model 456  :  0.899\n",
      "model 457  :  0.8878\n",
      "model 458  :  0.8882\n",
      "model 459  :  0.8761\n",
      "model 460  :  0.9178\n",
      "model 461  :  0.8892\n",
      "model 462  :  0.8979\n",
      "model 463  :  0.8882\n",
      "model 464  :  0.9074\n",
      "model 465  :  0.8633\n",
      "model 466  :  0.9179\n",
      "model 467  :  0.8995\n",
      "model 468  :  0.8956\n",
      "model 469  :  0.885\n",
      "model 470  :  0.8743\n",
      "model 471  :  0.9264\n",
      "model 472  :  0.9043\n",
      "model 473  :  0.918\n",
      "model 474  :  0.8806\n",
      "model 475  :  0.904\n",
      "model 476  :  0.857\n",
      "model 477  :  0.8406\n",
      "model 478  :  0.8985\n",
      "model 479  :  0.8885\n",
      "model 480  :  0.8987\n",
      "model 481  :  0.8934\n",
      "model 482  :  0.8887\n",
      "model 483  :  0.8894\n",
      "model 484  :  0.9044\n",
      "model 485  :  0.8761\n",
      "model 486  :  0.9064\n",
      "model 487  :  0.8976\n",
      "model 488  :  0.867\n",
      "model 489  :  0.8767\n",
      "model 490  :  0.8956\n",
      "model 491  :  0.8513\n",
      "model 492  :  0.8994\n",
      "model 493  :  0.85\n",
      "model 494  :  0.8743\n",
      "model 495  :  0.8175\n",
      "model 496  :  0.8582\n",
      "model 497  :  0.8584\n",
      "model 498  :  0.8602\n",
      "model 499  :  0.8753\n",
      "model 500  :  0.9175\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(weights)):\n",
    "    model_new = model = torch.load(weights[j]).to(device)\n",
    "# Make predictions\n",
    "    model_new.eval()\n",
    "    prediction = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        x_categ_val = torch.Tensor([[X_test[i][0][0],X_test[i][0][1]]]).long().to(device)\n",
    "        x_num_val = torch.Tensor([[X_test[i][0][2],X_test[i][0][3],X_test[i][0][4],X_test[i][0][5],X_test[i][0][6],X_test[i][0][7],X_test[i][0][8],X_test[i][0][9],X_test[i][0][10]]]).to(device)\n",
    "#     inputs = torch.Tensor(X_test[i]).to(device)\n",
    "        y_pred = model(x_categ_val,x_num_val)\n",
    "#     y_pred = model_best(x_categ_val,inputs)\n",
    "        y_pred = scaler.inverse_transform(y_pred[0][0].data.detach().cpu().numpy().reshape(-1, 1))\n",
    "        prediction.append(y_pred[0][0])\n",
    "    if len(y_pred.shape)>=3:\n",
    "        y_pred = tf.squeeze(y_pred, axis=-1)\n",
    "        y_pred = tf.squeeze(y_pred, axis=-1)\n",
    "    y_pred = np.array(prediction)\n",
    "    score = r2_score(y_test,y_pred)\n",
    "    print(\"model\",int(j+1),\" : \",round(score,4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
